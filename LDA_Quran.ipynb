{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy+EQoVQVaFdLWyaQ1bob+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherimello/LDA-Quran-Topic/blob/main/LDA_Quran.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CrISL5A_bo0",
        "outputId": "07c1a4e1-6ab4-4a49-ac1b-c102aff7d3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (71.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn gensim pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "def read_specific_column(csv_file, column_name):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if column_name in df.columns:\n",
        "        column_data = df[column_name].astype(str).tolist()\n",
        "        return column_data\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' not found in the CSV file.\")\n",
        "        return []\n",
        "\n",
        "# Load the Tafseer data\n",
        "csv_file_path = '/content/main_df.csv'\n",
        "column_name_to_read = 'Tafaseer2'\n",
        "column_data = read_specific_column(csv_file_path, column_name_to_read)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQZvYy5c_hiJ",
        "outputId": "6088575e-7489-41e8-f067-0d0bccfabe37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords if not already done\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the column data\n",
        "processed_data = [preprocess_text(doc) for doc in column_data]\n",
        "\n",
        "# Create a dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary(processed_data)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_data]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzpdT_CA_keQ",
        "outputId": "983fb197-f1dc-4841-d8ee-e5459b2d04c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10, random_state=100)\n",
        "\n",
        "# Display the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IswACI-P_m9i",
        "outputId": "f75023bb-854c-4150-f741-4e02edaa61c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.019*\"moses\" + 0.019*\"pharaoh\" + 0.015*\"hand\" + 0.014*\"said\" + 0.014*\"folk\"')\n",
            "(1, '0.060*\"allah\" + 0.043*\"said\" + 0.023*\"saying\" + 0.022*\"also\" + 0.018*\"night\"')\n",
            "(2, '0.024*\"day\" + 0.022*\"ye\" + 0.020*\"earth\" + 0.015*\"people\" + 0.014*\"land\"')\n",
            "(3, '0.039*\"lord\" + 0.036*\"thou\" + 0.028*\"us\" + 0.019*\"said\" + 0.019*\"allah\"')\n",
            "(4, '0.049*\"allah\" + 0.024*\"muhammad\" + 0.022*\"pbuh\" + 0.019*\"day\" + 0.015*\"al\"')\n",
            "(5, '0.048*\"allah\" + 0.020*\"good\" + 0.016*\"wealth\" + 0.015*\"reward\" + 0.013*\"also\"')\n",
            "(6, '0.046*\"ibn\" + 0.036*\"prayer\" + 0.033*\"hearts\" + 0.016*\"al\" + 0.014*\"pbuh\"')\n",
            "(7, '0.050*\"said\" + 0.049*\"also\" + 0.044*\"life\" + 0.042*\"fire\" + 0.040*\"means\"')\n",
            "(8, '0.045*\"said\" + 0.017*\"also\" + 0.013*\"means\" + 0.013*\"messengers\" + 0.012*\"people\"')\n",
            "(9, '0.058*\"allah\" + 0.023*\"unto\" + 0.022*\"muhammad\" + 0.020*\"lord\" + 0.019*\"said\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the topics using pyLDAvis\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(lda_vis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "uYr4kG0T_pYd",
        "outputId": "5870e63b-a996-471d-fa89-3e4d35a85436"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el3951351703448004801806820287\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el3951351703448004801806820287_data = {\"mdsDat\": {\"x\": [0.15987284089316392, 0.10557647173383795, 0.10281430624865767, 0.07103707672502921, 0.02835502989272424, -0.06368878797248263, 0.06643638092291981, -0.0015879789549956818, -0.24171851028332444, -0.22709682920552987], \"y\": [-0.02303246589495552, -0.08506260003754545, -0.0014611245440065777, -0.0450075712956782, 0.07527511685427465, 0.10133614652292598, -0.04988965233636007, 0.1432417796270938, 0.15970003833553256, -0.27509966723128126], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [18.74391425290782, 18.726528711093575, 14.127188569789991, 9.230881656379479, 9.08780956747702, 8.220285489124, 7.874226224096503, 7.138831612708578, 4.6022848161242464, 2.2480491002987883]}, \"tinfo\": {\"Term\": [\"ibn\", \"said\", \"allah\", \"also\", \"lord\", \"life\", \"fire\", \"thou\", \"means\", \"day\", \"world\", \"al\", \"us\", \"pbuh\", \"oneness\", \"divine\", \"earth\", \"hearts\", \"prophet\", \"muhammad\", \"hereafter\", \"created\", \"good\", \"saying\", \"wealth\", \"reward\", \"abbas\", \"authority\", \"night\", \"interpretation\", \"torah\", \"convey\", \"profession\", \"loss\", \"inherit\", \"straight\", \"upright\", \"pen\", \"mary\", \"surrendered\", \"serve\", \"profess\", \"invented\", \"writing\", \"gospel\", \"recited\", \"utter\", \"body\", \"teacheth\", \"tremendousness\", \"divinity\", \"perfectly\", \"pity\", \"voice\", \"descend\", \"follows\", \"disbelieveth\", \"event\", \"path\", \"declare\", \"oneness\", \"divine\", \"mother\", \"jesus\", \"description\", \"witness\", \"belief\", \"child\", \"scripture\", \"qur\", \"prohibitions\", \"religion\", \"believe\", \"abraham\", \"commands\", \"unto\", \"thee\", \"allah\", \"lord\", \"muhammad\", \"follow\", \"revelations\", \"worship\", \"guidance\", \"faith\", \"father\", \"said\", \"lo\", \"e\", \"pbuh\", \"also\", \"upon\", \"means\", \"hath\", \"save\", \"ye\", \"messenger\", \"people\", \"say\", \"revealed\", \"prophet\", \"one\", \"thou\", \"jahl\", \"banu\", \"badr\", \"calamity\", \"abd\", \"woe\", \"thinketh\", \"bint\", \"exhort\", \"courses\", \"gehenna\", \"expedition\", \"military\", \"seize\", \"clarification\", \"createth\", \"independent\", \"idol\", \"uncle\", \"counted\", \"nadr\", \"join\", \"finish\", \"umm\", \"resemble\", \"makhzumi\", \"speaks\", \"oppose\", \"happy\", \"abi\", \"hypocrites\", \"host\", \"choose\", \"disbelieve\", \"abu\", \"painful\", \"think\", \"jews\", \"proud\", \"goeth\", \"disbelievers\", \"truly\", \"severe\", \"al\", \"doom\", \"walid\", \"pbuh\", \"judgement\", \"medina\", \"day\", \"christians\", \"say\", \"muhammad\", \"mecca\", \"ibn\", \"prophet\", \"worship\", \"allah\", \"idolaters\", \"idols\", \"ye\", \"e\", \"believers\", \"among\", \"would\", \"unto\", \"people\", \"qur\", \"religion\", \"punishment\", \"disbelief\", \"lo\", \"believe\", \"acts\", \"poor\", \"believer\", \"abundant\", \"pleasure\", \"alms\", \"sees\", \"winter\", \"summer\", \"needy\", \"fig\", \"weight\", \"spend\", \"refuse\", \"usayd\", \"wine\", \"yemen\", \"charity\", \"syria\", \"private\", \"deed\", \"historic\", \"performing\", \"atom\", \"rank\", \"habitations\", \"fulfil\", \"privately\", \"spending\", \"purified\", \"reward\", \"wealth\", \"small\", \"love\", \"good\", \"works\", \"obedience\", \"due\", \"giving\", \"honour\", \"beneath\", \"gardens\", \"whoso\", \"give\", \"evil\", \"goodness\", \"allah\", \"better\", \"paradise\", \"one\", \"also\", \"garden\", \"means\", \"believers\", \"lord\", \"said\", \"al\", \"e\", \"ye\", \"shall\", \"hereafter\", \"hath\", \"ibn\", \"upon\", \"believe\", \"pbuh\", \"abbas\", \"authority\", \"narration\", \"interpretation\", \"sun\", \"creation\", \"moon\", \"dominion\", \"reply\", \"belongeth\", \"mim\", \"snorting\", \"hours\", \"favours\", \"create\", \"recorded\", \"horses\", \"protection\", \"ha\", \"glorified\", \"describe\", \"final\", \"succour\", \"glory\", \"stores\", \"beings\", \"worshippers\", \"enshrouds\", \"muzdalifah\", \"begotten\", \"heavens\", \"swore\", \"night\", \"verses\", \"beginning\", \"created\", \"merit\", \"sovereignty\", \"saying\", \"swears\", \"says\", \"power\", \"regarding\", \"heaven\", \"said\", \"allah\", \"earth\", \"praise\", \"ibn\", \"angels\", \"also\", \"quraysh\", \"day\", \"lord\", \"means\", \"none\", \"say\", \"one\", \"son\", \"muhammad\", \"noah\", \"humankind\", \"joseph\", \"ears\", \"bosom\", \"forsaken\", \"language\", \"ascent\", \"entire\", \"previous\", \"trumpet\", \"weighed\", \"king\", \"dilate\", \"conjunction\", \"thrown\", \"judges\", \"seeth\", \"creating\", \"disobedient\", \"bread\", \"listen\", \"troops\", \"extent\", \"dealt\", \"moves\", \"bewitched\", \"relate\", \"humans\", \"records\", \"blind\", \"messengers\", \"jinn\", \"heart\", \"rebellious\", \"warner\", \"portent\", \"hud\", \"sign\", \"led\", \"understanding\", \"warning\", \"sent\", \"truth\", \"saw\", \"prophethood\", \"years\", \"said\", \"two\", \"back\", \"folk\", \"see\", \"yes\", \"slave\", \"punishment\", \"came\", \"thee\", \"eyes\", \"know\", \"gabriel\", \"also\", \"means\", \"people\", \"denied\", \"among\", \"one\", \"way\", \"unto\", \"upon\", \"muhammad\", \"us\", \"prophet\", \"e\", \"lo\", \"allah\", \"pbuh\", \"land\", \"hour\", \"rain\", \"register\", \"sky\", \"cattle\", \"camels\", \"vegetation\", \"haply\", \"distracted\", \"contains\", \"green\", \"tortured\", \"reflect\", \"lands\", \"crops\", \"bounties\", \"hills\", \"rivalry\", \"soon\", \"clouds\", \"quite\", \"rebelled\", \"surface\", \"arabic\", \"travel\", \"ships\", \"step\", \"wool\", \"praises\", \"egypt\", \"term\", \"portents\", \"appointed\", \"coming\", \"sea\", \"beasts\", \"eat\", \"earth\", \"like\", \"signs\", \"made\", \"may\", \"day\", \"food\", \"darkness\", \"ye\", \"water\", \"thereof\", \"created\", \"therein\", \"people\", \"safe\", \"ward\", \"hath\", \"mecca\", \"judgement\", \"evil\", \"see\", \"allah\", \"would\", \"take\", \"lo\", \"things\", \"art\", \"city\", \"refuge\", \"wilt\", \"il\", \"sahmi\", \"daybreak\", \"wretched\", \"forms\", \"denies\", \"poured\", \"waiting\", \"hymn\", \"husbands\", \"relatives\", \"posterity\", \"falaq\", \"belieth\", \"inheritance\", \"eats\", \"didst\", \"grateful\", \"supplications\", \"flee\", \"please\", \"wa\", \"discourse\", \"ways\", \"rooms\", \"store\", \"seek\", \"wives\", \"thou\", \"orphan\", \"us\", \"bestow\", \"belong\", \"hast\", \"house\", \"thy\", \"lord\", \"hamstrung\", \"mercy\", \"therefore\", \"ask\", \"lo\", \"fear\", \"forgiveness\", \"give\", \"let\", \"said\", \"allah\", \"away\", \"make\", \"among\", \"without\", \"towards\", \"ever\", \"unto\", \"turn\", \"thee\", \"bridge\", \"begat\", \"begetter\", \"hubahib\", \"qurt\", \"immortal\", \"removed\", \"leads\", \"abundance\", \"brightness\", \"fire\", \"astonishment\", \"hardships\", \"iron\", \"flaming\", \"scales\", \"tightening\", \"sparks\", \"stopping\", \"destitute\", \"forelock\", \"toil\", \"samad\", \"lifted\", \"puss\", \"comfort\", \"keepers\", \"abuse\", \"raging\", \"scale\", \"life\", \"world\", \"olive\", \"hell\", \"abide\", \"portion\", \"hereafter\", \"paradise\", \"age\", \"former\", \"live\", \"also\", \"means\", \"enter\", \"death\", \"taste\", \"said\", \"go\", \"die\", \"latter\", \"refers\", \"garden\", \"man\", \"e\", \"therein\", \"torment\", \"light\", \"used\", \"disbeliever\", \"lo\", \"good\", \"says\", \"disbelievers\", \"old\", \"would\", \"unto\", \"save\", \"thee\", \"day\", \"pharaoh\", \"salih\", \"camel\", \"siddiq\", \"easy\", \"thamud\", \"shaken\", \"fruit\", \"stones\", \"palm\", \"stopped\", \"sahm\", \"ishah\", \"earthquake\", \"throw\", \"drop\", \"drowning\", \"sperm\", \"shu\", \"halter\", \"stands\", \"tree\", \"threw\", \"ayb\", \"manaf\", \"upper\", \"baked\", \"staff\", \"bones\", \"building\", \"neck\", \"inhabitants\", \"hand\", \"dead\", \"moses\", \"destroyed\", \"tribe\", \"mount\", \"forth\", \"denied\", \"graves\", \"folk\", \"found\", \"right\", \"left\", \"mountain\", \"water\", \"living\", \"people\", \"said\", \"sin\", \"trees\", \"wife\", \"brought\", \"means\", \"e\", \"khalaf\", \"black\", \"labid\", \"whispers\", \"admonished\", \"greeting\", \"arouse\", \"sam\", \"spent\", \"treasures\", \"intense\", \"greet\", \"prostrate\", \"erring\", \"couches\", \"mu\", \"registers\", \"enjoins\", \"bow\", \"safwan\", \"cooking\", \"opposition\", \"untied\", \"envier\", \"envieth\", \"dissuadeth\", \"leapeth\", \"scouring\", \"shakes\", \"yieldeth\", \"magic\", \"prayer\", \"ubayy\", \"dawn\", \"enters\", \"hearts\", \"remembrance\", \"prostration\", \"umayyah\", \"pray\", \"ibn\", \"peace\", \"enmity\", \"prayers\", \"satan\", \"abdullah\", \"man\", \"al\", \"prophet\", \"pbuh\", \"revealed\", \"says\", \"two\", \"mankind\", \"muhammad\", \"thou\"], \"Freq\": [1120.0, 3506.0, 6314.0, 2007.0, 1744.0, 639.0, 569.0, 724.0, 1534.0, 1283.0, 494.0, 837.0, 670.0, 1380.0, 603.0, 596.0, 573.0, 267.0, 805.0, 1938.0, 393.0, 451.0, 668.0, 751.0, 434.0, 406.0, 288.0, 287.0, 342.0, 282.0, 125.09563200062321, 98.53169837626312, 97.90477202526418, 80.84922270004759, 66.38130063778684, 60.18236930615687, 55.00003443867166, 49.736470028508194, 49.22954820227138, 47.441826750088964, 46.2951805511182, 41.35985260819004, 36.71902053637963, 36.36885975022856, 36.080014800643994, 34.01636815693413, 31.479157402737055, 31.298989609490395, 30.834007909712557, 30.743332021135696, 29.256254124338025, 28.1688051572311, 28.125298166497114, 27.683699658011403, 26.11776607681679, 25.31452429811951, 25.20051234198812, 25.04267334640491, 54.40457889042947, 52.15994567853224, 576.1189822526788, 568.2916037749573, 102.22553967133376, 99.91858924344615, 75.16638268859467, 99.20764912227274, 139.76718358698542, 49.09337630853625, 305.04904365705517, 637.692240089758, 91.85078545651817, 449.37705106337205, 466.459123198898, 129.9633884308192, 117.07108780480684, 772.562513360889, 351.9785372050402, 1962.839010179006, 690.7467828027814, 742.1917671345321, 128.0859630171478, 130.34717200374385, 266.13576911124477, 143.66310717677317, 193.66049741336312, 95.695312756503, 638.9442235501647, 303.5659653874858, 326.35189472143173, 315.51927841819315, 361.46463983025785, 197.09665040335534, 276.8425022034563, 201.58451096897824, 167.50196930352723, 248.1684304412714, 171.20393158937662, 226.38450884770663, 198.818378988015, 154.90628118984623, 172.40618379870872, 171.1439744872441, 155.18058846638291, 203.91093402837214, 198.1513829975336, 116.14530926289336, 88.70779030415505, 83.13233003305136, 70.2639692693288, 53.51939987151299, 49.53002654124752, 41.15479149386085, 39.67453261135534, 38.2339700398979, 37.335078894467344, 36.689015350989685, 35.997935453196405, 35.30185722566649, 34.94944359254804, 33.59859986286241, 32.34458405931497, 30.85700227064508, 30.585183762028006, 30.586113592599105, 30.22197507061259, 29.766087380607242, 29.95239678670479, 28.707761678918956, 28.22580538456899, 28.157137200726453, 28.143183857156338, 28.02082678533819, 39.25116545132404, 147.07578188033045, 174.5801074001012, 39.04994540773508, 256.7876915051754, 394.7357543126116, 59.48715840689886, 52.53862478578947, 193.8838922431156, 43.11509404454159, 40.54658814030351, 435.5748232092229, 61.416403831837705, 108.75613011114034, 520.1775410120503, 140.8004377460261, 98.23562684946737, 734.4672831083998, 269.4695761210313, 70.81854062327879, 653.3881288297384, 95.2655518385863, 482.7439589270825, 811.552418827276, 290.75643455537903, 494.9555768274348, 381.70651324341003, 304.5598910067689, 1637.8110065803862, 155.77667341106806, 110.83820515661351, 497.50543119355046, 494.3964544040229, 240.15563552979845, 249.57738507661898, 210.73675355411078, 418.7882505789421, 355.5431995739528, 313.4062598268584, 242.14639885918814, 185.1095715606113, 168.1755128930782, 222.09137368809672, 176.58138526928226, 188.4612044295138, 149.1170862376396, 111.81574683776714, 97.72261894700637, 96.04969550370107, 91.06505100540176, 82.88418052594555, 70.96664099494882, 70.07925098321581, 69.55159992013893, 69.35709747863721, 66.32696632997458, 59.31504025167215, 57.79833793970996, 49.8484812858684, 47.46723817466932, 44.60702584587725, 42.412305710261556, 44.45083431195965, 40.546489925402454, 40.5272368001062, 44.37277406191147, 37.32604733069076, 36.720001566657395, 36.03210708211017, 32.797120312501356, 31.71182711507519, 30.230600277332975, 29.809402274598966, 29.802065072814276, 388.696809320758, 395.49166788034756, 89.84327519970715, 76.7550982948173, 504.57629452044046, 286.22562086082974, 164.6731852740861, 114.86412775900114, 122.56366122682331, 103.47591450536531, 73.37589510965347, 81.64228439038716, 91.31735721893051, 219.72842197827416, 264.8477319039019, 93.82161315004704, 1210.689936062071, 110.53441509245513, 144.4394735926427, 217.53260081842936, 318.12215070981927, 99.71719537181754, 237.86520341799707, 148.51664918574096, 230.08621862112759, 284.89837758906185, 169.4560226862671, 174.1801388062301, 169.59509506926793, 116.03129631462411, 112.97723268769161, 120.3872323001747, 124.95433999727597, 117.75496750371413, 115.9072002289329, 115.92588212391675, 287.6656024386682, 286.3534986613289, 279.08546980225077, 280.94155019513636, 102.99910219743052, 95.71963219195884, 60.29216879878214, 43.11939871651841, 37.706444605274406, 32.34935214270808, 30.986190605744746, 35.88254485403656, 28.245421053866817, 27.176824556082664, 26.87411765985197, 25.814780832272724, 25.76053981927419, 27.21884504777276, 22.793987481873653, 21.390779998600173, 20.823230676260305, 20.554056240698543, 20.33947672783613, 20.401718420792406, 19.80237137549743, 156.54164829331378, 19.637493935372287, 19.587091778964552, 18.817355585089132, 18.603903555044063, 153.3883786127756, 82.6570765461754, 292.6077754226523, 64.1617478459024, 28.12772770598463, 289.42835897077526, 32.29024922267213, 52.81771968928821, 383.8475064536051, 81.2395038101279, 269.4581667094443, 103.59346163005664, 174.8500043713207, 85.96032658254713, 716.5379419665122, 990.7432484058002, 202.21563298402288, 49.21755681932864, 275.3495139074149, 116.85385086466232, 358.93201597289925, 68.79245708597662, 217.73106521063514, 245.38187928847856, 213.8565314277035, 85.14327092113335, 143.37566592184984, 113.50339739671975, 77.77354040442455, 109.03879751195873, 109.5437863326664, 53.150470930141935, 71.36325537858654, 42.86213952772829, 31.56001701652304, 31.275836021873594, 30.2100401985082, 28.472583133109016, 28.258159359392224, 28.19679669021163, 27.852652435666638, 26.855907464737637, 67.06317337499682, 26.062408128868963, 26.051477504779918, 25.56926391265172, 23.455649439348687, 22.5826255769252, 19.449085441474896, 19.013153265538385, 18.833712819989525, 18.54502706021298, 18.052606348115898, 18.093661399018536, 17.941933065230003, 17.459353913125305, 17.200095731457147, 17.146664952183357, 16.747540096380863, 16.638637494631148, 35.46186521675299, 210.68852757189205, 127.09510523529684, 103.54436069914422, 44.76507340915725, 37.973475498639424, 30.56329108517615, 39.365051899605234, 65.62656074650928, 44.358974354428064, 48.688769529857815, 45.20784650694569, 187.9071040062873, 189.0388015352082, 53.701750606372215, 84.8131131981932, 67.95789664915047, 729.9225613004066, 152.00353952417683, 107.55844533195125, 163.11074431353646, 125.71939642206105, 57.59958335873528, 62.546188644099665, 151.48263178932757, 104.25580134007932, 184.9022923894373, 56.14932315779168, 128.15812318363245, 103.12758424676804, 276.9320621687421, 220.39886676437584, 192.37954753879757, 71.81643551195602, 115.10461615271495, 126.50266959837874, 87.85740529250539, 139.1230032888074, 102.13407600479765, 131.38627671445934, 101.27592395995987, 102.20237479374072, 104.69746633418899, 98.17196943114945, 113.33969258616466, 94.29244133302643, 212.3629194917639, 141.13691623785837, 123.37362901702855, 69.89849932319258, 63.34587064717976, 58.855080757441485, 56.41529697831432, 55.775886672389795, 52.48780966748652, 43.56972417836919, 42.51280636363968, 35.70047935789846, 33.91553340593478, 33.55213072044338, 32.952203708995775, 32.12926824418443, 31.526416926969496, 30.082181874401616, 29.8629622742111, 26.2381836654517, 25.37599740397179, 24.420173080727732, 23.923554317665925, 23.350135500782955, 22.019293934865587, 21.75774423370027, 21.611854442265447, 21.14886347332195, 20.159993461583465, 20.14050153421324, 54.94045488093078, 44.73229244162795, 44.031819600199015, 94.3322554425463, 88.45825170280084, 65.78194479460913, 38.67936872180712, 78.60638612708364, 301.96451619458736, 204.11524746076395, 72.55356918403406, 212.19552106974712, 189.71613119420357, 351.5337102633272, 63.596858059021734, 52.00913629081788, 326.74014398017266, 95.26580819795578, 72.73924365599368, 141.44813952633058, 94.75281814477368, 228.92339049539945, 62.06585826878712, 60.24826273570622, 137.7998596946613, 117.33472060953666, 96.3906498168452, 98.39623446673335, 80.18715767036645, 108.20053271306678, 71.16684203381169, 66.54664558614809, 70.4422273088454, 63.97219563200878, 102.48012023873274, 93.40020107494671, 91.53788958877199, 50.843441533371845, 42.212347494909274, 40.82306559418042, 37.0974560987153, 41.673554100962846, 28.168903894721552, 28.130327211549343, 27.86262970990915, 38.52794101171361, 26.92817290870395, 28.296072993208245, 26.01895988127146, 25.12616306040101, 24.067386199874285, 24.065993849523732, 25.723266899241587, 22.536721349212804, 22.2459576833529, 22.166661351243828, 21.751104160955045, 21.413723153618903, 20.578604940931392, 49.62697604653231, 19.3137486805933, 18.9226302084243, 18.798232613005062, 18.384749100904248, 168.4574994458993, 73.67573048709481, 505.6530800291933, 76.4587220946436, 396.0460462513232, 45.31488020850169, 45.95755877575979, 96.9640504741166, 54.448253087997, 213.87231213643312, 550.461738603608, 39.56354612695138, 90.30890614641113, 94.03665035911988, 78.95793158453574, 234.3669834267301, 97.01564002407218, 55.60443508371103, 110.94209320072927, 84.32022912062607, 271.8391830336575, 268.44010609066453, 84.04495287785976, 84.4679304376708, 99.74673193727452, 74.34929151710453, 72.39737469723234, 59.68148106635437, 70.82768114444598, 54.315286928927556, 57.6422553010406, 29.881917999811122, 29.610864248177656, 29.610864248177656, 29.6069546608755, 29.60464839323207, 27.889817253987097, 26.910784204840937, 24.957589386519206, 48.737121706180695, 23.055557216411778, 545.0231041711154, 22.733802967473103, 23.181876969262913, 21.78383906094543, 21.570296923898738, 20.968399955447783, 20.601791642549014, 20.599718693811145, 20.59644325532923, 20.496289619483157, 20.149858905013385, 20.00258980658409, 19.765597623894507, 16.59915795736164, 16.430985878008496, 50.87396093768909, 15.777805114988428, 15.424060750209724, 14.70884707099063, 14.624973411343479, 568.6697239832096, 424.0432553036592, 59.28873888253964, 197.46009903519058, 42.052298751684305, 36.99669086316407, 255.54845402637113, 200.92294644109455, 56.93236151723844, 29.28029024548197, 26.53150039370736, 626.2404340609909, 511.39268463456114, 82.6057568540447, 128.84042262116688, 41.55289459067107, 642.216533324294, 94.24977613750829, 89.81527514066354, 47.632099439299616, 78.4480319045557, 67.50064903027291, 98.16096077678783, 165.93263663728203, 71.37401207670742, 71.41644022230328, 50.513016590786215, 71.79196400953569, 49.45933345693261, 109.88488879413805, 82.2526556118903, 70.7163698374355, 65.93405616029573, 52.71549919899378, 58.921788069452575, 64.69909321062528, 56.57266153783189, 57.570566496916314, 58.63774324080819, 154.05461256673553, 99.26745296506272, 93.30192351341634, 60.70456339774834, 54.86020044677623, 51.80772718602859, 46.15749830086498, 41.00086400412105, 38.26323620267895, 33.36479423535754, 32.46235527133384, 32.42910544951334, 31.595852418528388, 26.25275338328023, 26.104157033077204, 25.976283486799655, 24.28142771884534, 23.85171205095716, 23.55243665668925, 23.483225654187347, 23.47117302767197, 51.82961475578052, 23.253685792727286, 22.895128831785897, 22.310960201418343, 21.90755204247781, 21.653899214306026, 20.06192466628697, 19.696149628378055, 19.446783678405257, 34.97451989170845, 60.98591219744234, 123.27409429829915, 94.76576417842512, 160.61167335723516, 109.43294285853035, 51.70193405152722, 35.543893681975845, 96.36542733795086, 77.0754319389386, 49.72590882696145, 116.70741454763011, 46.15050199222534, 82.44485583891658, 59.77845500366867, 38.92190356385323, 59.62511088282871, 49.99942824047106, 106.9662156759086, 119.84523968830905, 46.44471721177703, 43.28851036934696, 42.95936374552333, 47.89200046058404, 47.39375837938003, 43.573114342410236, 49.450462106342, 42.44803504417979, 28.232840515056342, 28.232348810213953, 26.007565255364913, 24.418467517172495, 22.789027374785128, 21.240912575831384, 20.532688049399738, 17.9643516399233, 17.35697558827263, 15.613792435496164, 48.16516825546077, 13.14405226655138, 13.048154750873062, 11.646823267004924, 10.838360658010435, 10.696325779918075, 10.522264149639566, 10.371938110584193, 9.833961205031, 9.708494519415172, 9.471415544393924, 9.470815513772084, 9.470815513772084, 9.470652554121255, 9.470275768801416, 9.470166501058664, 9.46719178095855, 9.468908980053708, 47.00535521964486, 146.88519030038464, 55.4676407713688, 40.95488757161046, 13.208808475545169, 133.89036197582809, 29.878725452727995, 22.622719986388866, 29.348460400708568, 55.52236883907513, 185.61294882853005, 46.81304399240005, 21.783170704581224, 33.067282525408345, 26.080891446458395, 36.868078064291865, 52.82294316504759, 66.72885346438282, 54.96911611469244, 55.89933878371886, 32.8179448400062, 32.217006165544944, 29.979423213381036, 26.491438614879407, 24.23346404120592, 22.86670732024933], \"Total\": [1120.0, 3506.0, 6314.0, 2007.0, 1744.0, 639.0, 569.0, 724.0, 1534.0, 1283.0, 494.0, 837.0, 670.0, 1380.0, 603.0, 596.0, 573.0, 267.0, 805.0, 1938.0, 393.0, 451.0, 668.0, 751.0, 434.0, 406.0, 288.0, 287.0, 342.0, 282.0, 126.09661950471458, 99.53236106894947, 98.90558397267674, 81.85027656288243, 67.38213435475208, 61.18308921227953, 56.000609703765626, 50.7370739718311, 50.23028316153106, 48.44244899110588, 47.29605880480278, 42.360388576836186, 37.719602743894015, 37.36939009154811, 37.080626676636975, 35.01738397361495, 32.483171466462984, 32.306438262174126, 31.8346901180633, 31.743889967663208, 30.256748567515338, 29.169373337832656, 29.12598030991267, 28.68434514135987, 27.118571767067728, 26.31528219518445, 26.20122031434615, 26.043350595662538, 56.64674717752285, 54.31756610579417, 603.7377887309729, 596.0391546674183, 107.63032461365877, 105.65474160839068, 79.06177835458065, 110.05296990341685, 161.71907862316644, 53.294186868954846, 427.98281709121756, 1071.0397036103814, 113.87266535078453, 741.9390376050683, 843.5724717727627, 179.26240790588616, 160.86019665764556, 1727.6061813626998, 668.5492848770073, 6314.52726329685, 1744.3075846525962, 1938.3990913488997, 184.15508590605194, 207.4554605918252, 603.49630229862, 245.01340533161164, 397.2714641607071, 133.29911511898536, 3506.289317073301, 1215.8948697491355, 1467.57438971831, 1380.7901362967705, 2007.0129188809963, 623.8988342831375, 1534.9340828107013, 666.5540396398369, 432.0725387719971, 1410.8863640356712, 495.0855572422556, 1314.200269122966, 982.0188881246252, 445.2069711382634, 805.7934734914884, 825.1729381176141, 724.0575314245012, 204.92454435520727, 199.166567685165, 117.15498539515062, 89.71779089459918, 84.14203975229431, 71.27363848032014, 54.52918968493184, 50.539818581249705, 42.16515235775066, 40.684193125086324, 39.24861101890143, 38.344644680795135, 37.69865208437478, 37.00752782453798, 36.311514323173895, 35.95995285440511, 34.60834253763614, 33.355059897250044, 31.866674799061148, 31.595067712655563, 31.59884698992849, 31.231643019184364, 30.775957117379072, 30.981288209661212, 29.717719724019098, 29.235490752209646, 29.166948819401235, 29.152804884398016, 29.031697866701062, 40.66993319088895, 164.54986628092203, 198.1747928030207, 41.06093586830815, 311.2865121931007, 505.70238238740205, 65.17730487772164, 57.18116260215426, 244.6154234582431, 46.31800079907813, 43.27668844910147, 627.713584375124, 69.46905664962675, 135.1885991130079, 837.557698902083, 185.7219447317893, 121.71397635139581, 1380.7901362967705, 417.9301503474057, 83.6579575512555, 1283.2973901616956, 123.4913015665202, 982.0188881246252, 1938.3990913488997, 553.2854271699755, 1120.6988511561915, 805.7934734914884, 603.49630229862, 6314.52726329685, 254.7800229281842, 160.0506887858729, 1410.8863640356712, 1467.57438971831, 517.9562994731026, 598.2165622548106, 461.4964496143743, 1727.6061813626998, 1314.200269122966, 1071.0397036103814, 741.9390376050683, 447.9346160243172, 348.9968031011837, 1215.8948697491355, 843.5724717727627, 189.60531653597025, 150.14125667889937, 112.87116138030947, 98.74693745587057, 97.07399276705677, 92.08908806025866, 83.90834872439231, 71.99127680866168, 71.10395275519654, 70.57583011317013, 70.38133032619456, 67.35151473297273, 60.339214702967936, 58.82344235201113, 50.87293769162038, 48.49147432952038, 45.63123247373082, 43.43635135930702, 45.570526082009124, 41.57072402786229, 41.55374813983901, 45.56044639847885, 38.35017725574027, 37.74404470901599, 37.08190344348095, 33.821195519230876, 32.73611889597209, 31.254664444211976, 30.83348004025063, 30.826143199994103, 406.3785715107155, 434.3945796471325, 95.46255917512984, 83.8544762168369, 668.8143573573674, 366.25287782683444, 203.99107653900836, 141.28977518004663, 155.72078645156378, 127.55958290228149, 85.84175202007913, 99.36125885511514, 119.33085461557829, 407.76232720341557, 553.8914571244445, 129.86978627812124, 6314.52726329685, 178.66881680600085, 346.50265041792034, 825.1729381176141, 2007.0129188809963, 170.61113466462132, 1534.9340828107013, 517.9562994731026, 1744.3075846525962, 3506.289317073301, 837.557698902083, 1467.57438971831, 1410.8863640356712, 444.72853524801235, 393.5634417940756, 666.5540396398369, 1120.6988511561915, 623.8988342831375, 843.5724717727627, 1380.7901362967705, 288.71299201137464, 287.40088245446447, 280.13284224848513, 282.2306064717753, 104.04657026030932, 96.77094366374598, 61.33954770208715, 44.16674866938309, 38.75457378313524, 33.41064556129991, 32.033512226165165, 37.14085793782133, 29.292803020264305, 28.22425040930745, 27.923924632164443, 26.863852086594395, 26.81007029733697, 28.363609879186363, 23.841299174969187, 22.444171092304348, 21.87063693173284, 21.60168706851519, 21.38688688730475, 21.454071963276004, 20.849776588168822, 164.82666356626547, 20.6851123936725, 20.634403438735827, 19.864771670289272, 19.65129642410995, 164.6334364725572, 91.22895363238456, 342.26165049414243, 71.34571895745239, 30.135354184067612, 451.26547190288636, 36.319954386337116, 64.21965412801832, 751.2219233143154, 113.2358465908879, 532.4590997527907, 181.13411261416826, 391.77398608926114, 144.32657092016075, 3506.289317073301, 6314.52726329685, 573.8967901933316, 70.44260799983135, 1120.6988511561915, 295.8514698712874, 2007.0129188809963, 131.93096715644418, 1283.2973901616956, 1744.3075846525962, 1534.9340828107013, 212.26036219969905, 982.0188881246252, 825.1729381176141, 194.9488325579898, 1938.3990913488997, 110.60391503177084, 54.16024781811086, 73.05602002887667, 43.88543802812446, 32.568104089321146, 32.28401815551801, 31.227350206151566, 29.481244264673318, 29.26658362306848, 29.205655555696094, 28.860804365590095, 27.86388236055287, 69.63704705999565, 27.07039147401826, 27.05957182760106, 26.57755634058442, 24.46372118971813, 23.590919410891196, 20.457280646822756, 20.022137949528915, 19.842356418519675, 19.553172709205022, 19.060783789537844, 19.10429201825689, 18.950959339438896, 18.46775094798556, 18.208154057385272, 18.155248861675513, 17.75563020379536, 17.64677024208586, 38.29278762406475, 240.7720553932875, 146.6977122115483, 120.22576532299253, 50.5841745439747, 42.81693692604989, 33.941035090172946, 45.51813448350961, 81.27311389422067, 54.301283105986144, 61.248331625946136, 57.70054425768955, 341.7736959472754, 379.13912249968087, 73.66559680312818, 145.92498109317714, 106.57420516758602, 3506.289317073301, 350.1223382589455, 215.50125385184197, 414.24169271093956, 279.89605108764334, 87.19610707864035, 101.14621383864333, 447.9346160243172, 247.71549008126024, 668.5492848770073, 88.25179950178396, 433.2095642954981, 285.2972704484614, 2007.0129188809963, 1534.9340828107013, 1314.200269122966, 152.87353420602733, 598.2165622548106, 825.1729381176141, 337.1033544176494, 1727.6061813626998, 623.8988342831375, 1938.3990913488997, 670.0769780365216, 805.7934734914884, 1467.57438971831, 1215.8948697491355, 6314.52726329685, 1380.7901362967705, 213.37136157890257, 142.18998083714567, 124.58442098734503, 70.90547067638357, 64.36695707017314, 59.86090897616495, 57.457649715659954, 56.80715066856337, 53.49363359195568, 44.57681249851733, 43.52495263543951, 36.70695629901425, 34.9217326020092, 34.557981374985026, 33.95800549387226, 33.135061564549034, 32.53230310701731, 31.088133290676552, 30.8690052429394, 27.244334541311552, 26.38241506705228, 25.42670540963439, 24.929447561031292, 24.35650535848824, 23.02511876285213, 22.767572215191898, 22.617641933858625, 22.15575400896613, 21.16578921158947, 21.146764773388863, 59.93917373797496, 49.522529420918865, 49.750928198666486, 121.96914893296814, 113.62399609403938, 82.87253567292893, 44.47094500916079, 104.70925135912502, 573.8967901933316, 356.1621677822527, 112.46452948795283, 530.1446148608935, 485.37622958426454, 1283.2973901616956, 96.56578361700467, 71.05808306274369, 1410.8863640356712, 193.87958628046388, 130.66885961748338, 451.26547190288636, 228.27520192139815, 1314.200269122966, 102.43940719668176, 97.04531419618382, 666.5540396398369, 553.2854271699755, 417.9301503474057, 553.8914571244445, 279.89605108764334, 6314.52726329685, 461.4964496143743, 218.9773374154366, 1215.8948697491355, 326.62722705008326, 103.49891566151446, 94.41870653838289, 92.5576950692874, 51.86178931180094, 43.23020346589631, 41.853343081869724, 38.118698647844134, 42.93211413337368, 29.187969438485677, 29.14821296956545, 28.8808960017408, 39.98437770827672, 27.94611574957955, 29.42681133894562, 27.071033241669838, 26.143927611251012, 25.08520286846239, 25.085272974499134, 26.815101371032306, 23.55461062592018, 23.267016049563313, 23.18448854923802, 22.769710994281294, 22.431625920907535, 21.597167729499542, 52.201229076057146, 20.3315523499177, 19.94099662018911, 19.816472126482278, 19.40378288518398, 179.0502880754031, 78.63455484158415, 724.0575314245012, 91.32169000934394, 670.0769780365216, 54.40367800818862, 56.27282746326424, 152.9414696612538, 75.948221115587, 474.5368269141115, 1744.3075846525962, 54.3575521125475, 206.78265486905016, 233.97087654708722, 177.0681056095291, 1215.8948697491355, 262.9733964971599, 100.91684378534791, 407.76232720341557, 244.029218582212, 3506.289317073301, 6314.52726329685, 322.56964286542836, 341.4696740916276, 598.2165622548106, 268.28415146088463, 272.2983266434735, 149.8492917976494, 1727.6061813626998, 164.5131178783181, 668.5492848770073, 30.92102192199624, 30.649762092443243, 30.649762092443243, 30.645951674799193, 30.64428176426646, 28.92874488864437, 27.94982246061413, 25.996811182110605, 50.92135139255402, 24.09472287656243, 569.6928128487006, 23.773800163845337, 24.242998298711324, 22.822930623615846, 22.60925602166579, 22.007355656079998, 21.640746967215698, 21.63871216951672, 21.63546868417698, 21.537266611285283, 21.18928304906033, 21.041679584260667, 20.80455852094816, 17.638167796985314, 17.47003131745239, 54.16491367796303, 16.816743209055314, 16.46325759683654, 15.74777924959339, 15.66397206279389, 639.4976896866747, 494.8973357155155, 66.01789033187907, 257.93817740513185, 48.509753461220384, 42.262472544793695, 393.5634417940756, 346.50265041792034, 74.62922156659562, 33.854814123019224, 30.09701925084416, 2007.0129188809963, 1534.9340828107013, 141.03207698611396, 283.36999000560934, 57.9654899278235, 3506.289317073301, 220.14054400318918, 229.22604592371601, 81.51786003636451, 225.4648796648261, 170.61113466462132, 385.6049344744837, 1467.57438971831, 228.27520192139815, 234.16717257795017, 109.97990185662694, 271.728264344157, 122.14531345461522, 1215.8948697491355, 668.8143573573674, 532.4590997527907, 627.713584375124, 181.37855011735172, 461.4964496143743, 1727.6061813626998, 432.0725387719971, 668.5492848770073, 1283.2973901616956, 155.08431676956093, 100.29719639046951, 94.3321109563394, 61.734695053648124, 55.89017533431179, 52.83742851301344, 47.18734662347796, 42.03065770339233, 39.378067224374696, 34.394460573082874, 33.49222366190225, 33.458772867686875, 32.62607340547208, 27.28244778137062, 27.145985990229452, 27.02232972927235, 25.313363177623064, 24.89436350519803, 24.583502761206685, 24.512894158073617, 24.500923868373857, 54.15543529506748, 24.299780137431192, 23.925926683825033, 23.340651187881942, 22.94024525283901, 22.683555311566636, 21.091612214361504, 20.72691748044816, 20.47672893963904, 37.56767180735684, 67.44492023648984, 158.09145191842964, 139.4903001355542, 278.8346065604209, 183.27693656715448, 70.60195856155698, 43.45203572535188, 198.68130107459794, 152.87353420602733, 85.48633707674546, 414.24169271093956, 79.3269183912138, 298.27369713895587, 175.3858108209904, 69.41463047382818, 193.87958628046388, 128.57943392359294, 1314.200269122966, 3506.289317073301, 126.83877661539032, 99.34378452711267, 101.1816774474342, 194.3132698519677, 1534.9340828107013, 1467.57438971831, 50.493930605468194, 43.494588883259425, 29.276272549726862, 29.27578539055066, 27.051182366591345, 25.461964770561536, 23.832614510669863, 22.28435296758243, 21.576255310217363, 19.00787681319767, 18.400504135876407, 16.657233142980655, 51.817365585689444, 14.188108691171456, 14.0917947589256, 12.707598238738953, 11.88183048229733, 11.739893708195622, 11.565743804079263, 11.415469020952521, 10.878008273989982, 10.760016863038222, 10.514846311600575, 10.514255373230785, 10.514255373230785, 10.514090243666098, 10.513720531878029, 10.513605849152052, 10.510779443544575, 10.51273734684814, 53.784469898221545, 174.89705594500305, 68.68743061026083, 54.430358817060274, 15.292166441450828, 267.0221969005915, 43.659968285311734, 35.42137901604613, 51.321650251920055, 127.89250452999303, 1120.6988511561915, 125.7849433085625, 41.28216717081593, 103.13207469172656, 69.01981211095395, 158.44859696162604, 385.6049344744837, 837.557698902083, 805.7934734914884, 1380.7901362967705, 445.2069711382634, 532.4590997527907, 350.1223382589455, 248.30556554030963, 1938.3990913488997, 724.0575314245012], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.5971, -5.8358, -5.8422, -6.0336, -6.2308, -6.3288, -6.4189, -6.5195, -6.5297, -6.5667, -6.5912, -6.7039, -6.8229, -6.8325, -6.8405, -6.8994, -6.9769, -6.9826, -6.9976, -7.0005, -7.0501, -7.088, -7.0895, -7.1054, -7.1636, -7.1948, -7.1994, -7.2056, -6.4298, -6.4719, -4.0699, -4.0836, -5.799, -5.8219, -6.1065, -5.829, -5.4862, -6.5325, -4.7057, -3.9684, -5.9061, -4.3184, -4.281, -5.559, -5.6634, -3.7765, -4.5626, -2.8441, -3.8884, -3.8166, -5.5735, -5.556, -4.8422, -5.4587, -5.1601, -5.865, -3.9664, -4.7106, -4.6382, -4.672, -4.5361, -5.1425, -4.8028, -5.12, -5.3052, -4.9121, -5.2834, -5.004, -5.1338, -5.3834, -5.2764, -5.2837, -5.3816, -5.1076, -5.1363, -5.6704, -5.9399, -6.0049, -6.173, -6.4452, -6.5227, -6.7079, -6.7446, -6.7816, -6.8054, -6.8228, -6.8418, -6.8614, -6.8714, -6.9108, -6.9488, -6.9959, -7.0048, -7.0047, -7.0167, -7.0319, -7.0257, -7.0681, -7.0851, -7.0875, -7.088, -7.0923, -6.7553, -5.4343, -5.2629, -6.7604, -4.877, -4.4471, -6.3395, -6.4637, -5.158, -6.6614, -6.7228, -4.3486, -6.3076, -5.7362, -4.1711, -5.4779, -5.8379, -3.8261, -4.8288, -6.1652, -3.9431, -5.8686, -4.2458, -3.7263, -4.7528, -4.2208, -4.4806, -4.7064, -3.0242, -5.3769, -5.7172, -4.2157, -4.222, -4.944, -4.9055, -5.0747, -4.3879, -4.5516, -4.6778, -4.9357, -5.2043, -5.3003, -5.0222, -5.2515, -4.9046, -5.1387, -5.4266, -5.5613, -5.5786, -5.6319, -5.726, -5.8812, -5.8938, -5.9014, -5.9042, -5.9489, -6.0606, -6.0865, -6.2345, -6.2834, -6.3456, -6.396, -6.3491, -6.441, -6.4415, -6.3508, -6.5238, -6.5401, -6.559, -6.6531, -6.6868, -6.7346, -6.7486, -6.7489, -4.1806, -4.1633, -5.6454, -5.8028, -3.9197, -4.4867, -5.0395, -5.3997, -5.3348, -5.5041, -5.8479, -5.7411, -5.6291, -4.7511, -4.5643, -5.6021, -3.0445, -5.4381, -5.1706, -4.7611, -4.381, -5.5411, -4.6717, -5.1428, -4.705, -4.4913, -5.0109, -4.9834, -5.01, -5.3896, -5.4163, -5.3527, -5.3155, -5.3748, -5.3907, -5.3905, -4.0561, -4.0607, -4.0864, -4.0798, -5.0832, -5.1565, -5.6187, -5.9539, -6.0881, -6.2413, -6.2844, -6.1377, -6.377, -6.4155, -6.4267, -6.467, -6.4691, -6.414, -6.5914, -6.6549, -6.6818, -6.6948, -6.7053, -6.7023, -6.7321, -4.6646, -6.7405, -6.743, -6.7831, -6.7945, -4.6849, -5.3032, -4.0391, -5.5565, -6.3811, -4.05, -6.2431, -5.7511, -3.7677, -5.3205, -4.1215, -5.0774, -4.554, -5.264, -3.1435, -2.8194, -4.4086, -5.8217, -4.0999, -4.957, -3.8348, -5.4868, -4.3346, -4.2151, -4.3526, -5.2736, -4.7524, -4.9861, -5.3641, -5.0262, -5.006, -5.7292, -5.4345, -5.9443, -6.2504, -6.2594, -6.2941, -6.3533, -6.3609, -6.3631, -6.3754, -6.4118, -5.4966, -6.4418, -6.4422, -6.4609, -6.5472, -6.5851, -6.7345, -6.7572, -6.7666, -6.7821, -6.809, -6.8067, -6.8151, -6.8424, -6.8574, -6.8605, -6.884, -6.8906, -6.1338, -4.3519, -4.8573, -5.0623, -5.9009, -6.0654, -6.2825, -6.0294, -5.5183, -5.91, -5.8168, -5.891, -4.4663, -4.4603, -5.7188, -5.2618, -5.4834, -3.1093, -4.6784, -5.0242, -4.6079, -4.8682, -5.6488, -5.5664, -4.6818, -5.0554, -4.4825, -5.6743, -4.849, -5.0663, -4.0785, -4.3068, -4.4428, -5.4282, -4.9564, -4.862, -5.2266, -4.7669, -5.076, -4.8241, -5.0844, -5.0753, -5.0512, -5.1156, -4.9719, -5.1559, -4.2437, -4.6522, -4.7867, -5.3549, -5.4533, -5.5269, -5.5692, -5.5806, -5.6414, -5.8276, -5.8521, -6.0268, -6.0781, -6.0889, -6.1069, -6.1322, -6.1511, -6.198, -6.2053, -6.3347, -6.3681, -6.4065, -6.4271, -6.4513, -6.51, -6.522, -6.5287, -6.5504, -6.5983, -6.5992, -5.5957, -5.8013, -5.817, -5.0551, -5.1194, -5.4156, -5.9466, -5.2375, -3.8916, -4.2833, -5.3176, -4.2444, -4.3564, -3.7396, -5.4494, -5.6505, -3.8128, -5.0453, -5.3151, -4.65, -5.0507, -4.1686, -5.4738, -5.5035, -4.6762, -4.8369, -5.0335, -5.0129, -5.2176, -4.918, -5.3369, -5.404, -5.3472, -5.4435, -4.9293, -5.022, -5.0422, -5.6302, -5.8162, -5.8497, -5.9454, -5.8291, -6.2207, -6.2221, -6.2317, -5.9076, -6.2658, -6.2162, -6.3001, -6.335, -6.3781, -6.3781, -6.3115, -6.4438, -6.4568, -6.4604, -6.4793, -6.4949, -6.5347, -5.6544, -6.5981, -6.6186, -6.6252, -6.6474, -4.4323, -5.2593, -3.3331, -5.2222, -3.5774, -5.7453, -5.7312, -4.9846, -5.5617, -4.1936, -3.2482, -5.881, -5.0557, -5.0153, -5.19, -4.1021, -4.9841, -5.5407, -4.8499, -5.1243, -3.9537, -3.9663, -5.1276, -5.1226, -4.9563, -5.2502, -5.2768, -5.4699, -5.2987, -5.5641, -5.5047, -6.0636, -6.0728, -6.0728, -6.0729, -6.073, -6.1326, -6.1684, -6.2437, -5.5745, -6.323, -3.1601, -6.337, -6.3175, -6.3797, -6.3896, -6.4179, -6.4355, -6.4356, -6.4358, -6.4407, -6.4577, -6.465, -6.477, -6.6515, -6.6617, -5.5315, -6.7023, -6.725, -6.7724, -6.7782, -3.1176, -3.4111, -5.3785, -4.1754, -5.722, -5.8501, -3.9175, -4.158, -5.419, -6.084, -6.1826, -3.0212, -3.2238, -5.0468, -4.6023, -5.7339, -2.996, -4.9149, -4.9631, -5.5974, -5.0985, -5.2488, -4.8743, -4.3493, -5.193, -5.1924, -5.5387, -5.1871, -5.5597, -4.7615, -5.0511, -5.2022, -5.2722, -5.496, -5.3847, -5.2911, -5.4254, -5.4079, -5.3895, -3.9846, -4.4241, -4.4861, -4.9159, -5.0171, -5.0744, -5.1898, -5.3083, -5.3774, -5.5144, -5.5418, -5.5428, -5.5689, -5.7541, -5.7598, -5.7647, -5.8322, -5.85, -5.8627, -5.8656, -5.8661, -5.0739, -5.8754, -5.891, -5.9168, -5.9351, -5.9467, -6.0231, -6.0415, -6.0542, -5.4673, -4.9113, -4.2075, -4.4705, -3.9429, -4.3266, -5.0764, -5.4511, -4.4538, -4.6771, -5.1154, -4.2622, -5.19, -4.6098, -4.9313, -5.3603, -4.9338, -5.1099, -4.3494, -4.2357, -5.1836, -5.254, -5.2616, -5.153, -5.1634, -5.2475, -4.4044, -4.5571, -4.9649, -4.9649, -5.047, -5.1101, -5.1791, -5.2495, -5.2834, -5.417, -5.4514, -5.5573, -4.4308, -5.7294, -5.7368, -5.8504, -5.9223, -5.9355, -5.9519, -5.9663, -6.0196, -6.0324, -6.0571, -6.0572, -6.0572, -6.0572, -6.0573, -6.0573, -6.0576, -6.0574, -4.4551, -3.3158, -4.2896, -4.5929, -5.7245, -3.4084, -4.9083, -5.1865, -4.9262, -4.2886, -3.0817, -4.4592, -5.2243, -4.8069, -5.0442, -4.6981, -4.3385, -4.1048, -4.2986, -4.2819, -4.8144, -4.8329, -4.9049, -5.0286, -5.1177, -5.1757], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6663, 1.6642, 1.6641, 1.662, 1.6593, 1.6578, 1.6563, 1.6544, 1.6542, 1.6534, 1.6529, 1.6504, 1.6474, 1.6472, 1.6469, 1.6453, 1.6429, 1.6426, 1.6424, 1.6423, 1.6407, 1.6394, 1.6393, 1.6388, 1.6367, 1.6355, 1.6354, 1.6351, 1.6339, 1.6338, 1.6275, 1.6266, 1.6228, 1.6185, 1.6238, 1.5706, 1.5284, 1.5922, 1.3357, 1.1558, 1.4594, 1.1729, 1.0818, 1.3527, 1.3565, 0.8695, 1.0328, 0.5058, 0.748, 0.7143, 1.3112, 1.2096, 0.8556, 1.1405, 0.9558, 1.3429, -0.0282, 0.2867, 0.1709, 0.1981, -0.0399, 0.522, -0.0385, 0.4784, 0.7267, -0.0636, 0.6124, -0.0844, 0.0771, 0.6186, 0.1323, 0.1012, 0.134, 1.6703, 1.6701, 1.6666, 1.6639, 1.6632, 1.661, 1.6565, 1.655, 1.651, 1.6501, 1.649, 1.6485, 1.6481, 1.6476, 1.647, 1.6467, 1.6456, 1.6445, 1.643, 1.6427, 1.6427, 1.6424, 1.6419, 1.6415, 1.6407, 1.6401, 1.64, 1.64, 1.6398, 1.6397, 1.563, 1.5485, 1.625, 1.4828, 1.4275, 1.5839, 1.5906, 1.4428, 1.6036, 1.6101, 1.3098, 1.552, 1.4577, 1.1989, 1.3983, 1.4609, 1.044, 1.2364, 1.5086, 1.0002, 1.4157, 0.9651, 0.8046, 1.0318, 0.858, 0.9281, 0.9914, 0.3257, 1.1833, 1.3078, 0.6329, 0.5872, 0.9066, 0.801, 0.8914, 0.2581, 0.3679, 0.4463, 0.5555, 0.7915, 0.9452, -0.0249, 0.1114, 1.951, 1.9502, 1.9477, 1.9466, 1.9465, 1.9459, 1.9448, 1.9427, 1.9426, 1.9425, 1.9424, 1.9417, 1.9399, 1.9395, 1.9367, 1.9357, 1.9344, 1.9332, 1.9322, 1.9321, 1.9321, 1.9307, 1.93, 1.9296, 1.9284, 1.9263, 1.9253, 1.9238, 1.9233, 1.9233, 1.9126, 1.8632, 1.8964, 1.8686, 1.6753, 1.7105, 1.743, 1.75, 1.7176, 1.7478, 1.8002, 1.7607, 1.6895, 1.3388, 1.2193, 1.6319, 0.3054, 1.4769, 1.082, 0.6238, 0.1151, 1.42, 0.0925, 0.7079, -0.0686, -0.5531, 0.3592, -0.1742, -0.1615, 0.6135, 0.709, 0.2457, -0.2367, 0.2897, -0.0278, -0.5204, 2.379, 2.379, 2.3789, 2.378, 2.3725, 2.3717, 2.3654, 2.3586, 2.3552, 2.3503, 2.3494, 2.3481, 2.3462, 2.3448, 2.3443, 2.3428, 2.3427, 2.3414, 2.3377, 2.3345, 2.3335, 2.3329, 2.3324, 2.3323, 2.3311, 2.331, 2.3306, 2.3305, 2.3284, 2.3278, 2.3119, 2.2839, 2.2259, 2.2765, 2.3137, 1.9385, 2.265, 2.1872, 1.7112, 2.0505, 1.7015, 1.8239, 1.5759, 1.8644, 0.7947, 0.5305, 1.3395, 2.0241, 0.9789, 1.4537, 0.6613, 1.7314, 0.6087, 0.4213, 0.4117, 1.4691, 0.4585, 0.3989, 1.4637, -0.4953, 2.3886, 2.3794, 2.3748, 2.3746, 2.3668, 2.3665, 2.3651, 2.3634, 2.3632, 2.3631, 2.3627, 2.3614, 2.3606, 2.3603, 2.3603, 2.3596, 2.3562, 2.3546, 2.3477, 2.3465, 2.3461, 2.3453, 2.3439, 2.3439, 2.3435, 2.3421, 2.3413, 2.3411, 2.3398, 2.3394, 2.3214, 2.2648, 2.2548, 2.2489, 2.276, 2.2782, 2.2934, 2.253, 2.1844, 2.196, 2.1687, 2.1542, 1.8, 1.7023, 2.0821, 1.8556, 1.9483, 0.8289, 1.5639, 1.7033, 1.4662, 1.5979, 1.9836, 1.9176, 1.3141, 1.5328, 1.113, 1.9461, 1.1803, 1.3807, 0.4176, 0.4574, 0.4767, 1.6427, 0.7501, 0.5229, 1.0536, -0.1209, 0.5885, -0.2932, 0.5087, 0.3334, -0.2421, -0.1183, -1.622, -0.2858, 2.4938, 2.4911, 2.4888, 2.4843, 2.4826, 2.4816, 2.4803, 2.4802, 2.4796, 2.4757, 2.475, 2.4708, 2.4693, 2.469, 2.4685, 2.4677, 2.4672, 2.4657, 2.4654, 2.4609, 2.4597, 2.4582, 2.4574, 2.4564, 2.4539, 2.4532, 2.4531, 2.4521, 2.4499, 2.4498, 2.4115, 2.3968, 2.3764, 2.2416, 2.2482, 2.2676, 2.359, 2.2118, 1.8564, 1.9419, 2.0603, 1.5829, 1.5592, 1.2037, 2.0809, 2.1865, 1.0358, 1.788, 1.9128, 1.3384, 1.6193, 0.751, 1.9975, 2.0219, 0.9222, 0.9477, 1.0317, 0.7706, 1.2485, -1.5681, 0.6291, 1.3075, -0.3499, 0.8682, 2.5317, 2.5307, 2.5305, 2.5217, 2.5177, 2.5167, 2.5144, 2.5118, 2.506, 2.506, 2.5057, 2.5045, 2.5045, 2.5024, 2.5019, 2.5019, 2.5002, 2.5001, 2.5, 2.4974, 2.4967, 2.4967, 2.4958, 2.4951, 2.4933, 2.491, 2.4902, 2.4892, 2.4888, 2.4876, 2.4806, 2.4764, 2.1826, 2.3639, 2.0157, 2.3588, 2.3391, 2.0859, 2.2088, 1.7446, 1.3882, 2.2239, 1.7131, 1.6301, 1.734, 0.8952, 1.5444, 1.9455, 1.2399, 1.4789, -0.0155, -0.6164, 1.1966, 1.1447, 0.7503, 1.2583, 1.2168, 1.621, -0.6527, 1.4334, 0.0907, 2.6054, 2.6051, 2.6051, 2.6051, 2.6051, 2.603, 2.6017, 2.5988, 2.5958, 2.5955, 2.5954, 2.5949, 2.5949, 2.593, 2.5926, 2.5913, 2.5904, 2.5904, 2.5904, 2.5901, 2.5893, 2.589, 2.5884, 2.5789, 2.5783, 2.5769, 2.5759, 2.5744, 2.5714, 2.571, 2.5222, 2.4851, 2.5321, 2.3724, 2.4968, 2.5066, 2.2078, 2.0947, 2.369, 2.4945, 2.5135, 1.475, 1.5405, 2.1047, 1.8514, 2.3067, 0.9422, 1.7913, 1.7027, 2.1023, 1.5839, 1.7124, 1.2714, 0.4598, 1.477, 1.4521, 1.8616, 1.3086, 1.7356, 0.2358, 0.5439, 0.6208, 0.3862, 1.4039, 0.5814, -0.6451, 0.6066, 0.1875, -0.4462, 3.072, 3.0683, 3.0676, 3.0618, 3.06, 3.0589, 3.0566, 3.0538, 3.0499, 3.0482, 3.0474, 3.0474, 3.0465, 3.0401, 3.0395, 3.0391, 3.037, 3.0358, 3.0358, 3.0357, 3.0357, 3.0347, 3.0346, 3.0346, 3.0335, 3.0326, 3.0322, 3.0286, 3.0276, 3.027, 3.0071, 2.9779, 2.8299, 2.692, 2.527, 2.5629, 2.7671, 2.8777, 2.3551, 2.3938, 2.5368, 1.8118, 2.5369, 1.7927, 2.0023, 2.5001, 1.8995, 2.1341, 0.5701, -0.2975, 2.074, 2.2479, 2.222, 1.6781, -0.3991, -0.4383, 3.7742, 3.7708, 3.7588, 3.7588, 3.7558, 3.7533, 3.7503, 3.7472, 3.7455, 3.7386, 3.7367, 3.7304, 3.722, 3.7187, 3.7182, 3.7079, 3.7032, 3.702, 3.7006, 3.6992, 3.6942, 3.6923, 3.6906, 3.6906, 3.6906, 3.6906, 3.6906, 3.6906, 3.6905, 3.6905, 3.6604, 3.6206, 3.5813, 3.5107, 3.6487, 3.1048, 3.4158, 3.3467, 3.2362, 2.9607, 1.9971, 2.8067, 3.1558, 2.6576, 2.8219, 2.337, 1.8072, 1.2653, 1.1101, 0.5882, 1.1875, 0.9901, 1.3373, 1.5573, -0.5868, 0.3399]}, \"token.table\": {\"Topic\": [4, 2, 2, 3, 7, 10, 2, 3, 3, 8, 1, 5, 7, 2, 3, 8, 4, 8, 3, 8, 3, 10, 1, 7, 8, 2, 3, 4, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 10, 1, 3, 6, 7, 8, 9, 6, 10, 7, 5, 1, 2, 3, 4, 7, 8, 8, 3, 4, 1, 2, 4, 6, 7, 8, 9, 1, 2, 3, 5, 7, 8, 9, 2, 9, 2, 4, 6, 9, 8, 8, 4, 6, 4, 4, 5, 1, 2, 3, 7, 1, 2, 3, 5, 6, 9, 3, 1, 2, 3, 5, 6, 7, 8, 4, 7, 4, 3, 6, 8, 3, 7, 1, 2, 3, 4, 6, 7, 8, 5, 2, 10, 1, 4, 5, 1, 9, 5, 6, 10, 5, 8, 8, 1, 4, 5, 6, 9, 10, 9, 2, 1, 2, 5, 9, 9, 6, 6, 3, 1, 4, 2, 4, 1, 2, 7, 2, 6, 2, 3, 8, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 5, 6, 1, 10, 10, 2, 2, 4, 4, 6, 8, 2, 5, 4, 6, 4, 6, 10, 4, 10, 2, 3, 4, 5, 6, 8, 7, 6, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 1, 5, 9, 7, 1, 4, 1, 3, 8, 1, 5, 6, 7, 9, 10, 7, 2, 3, 7, 8, 5, 1, 2, 3, 5, 6, 8, 1, 2, 4, 6, 8, 2, 3, 5, 8, 1, 2, 4, 5, 6, 7, 8, 9, 1, 7, 5, 10, 6, 1, 3, 1, 4, 2, 5, 6, 8, 9, 9, 1, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 2, 4, 6, 8, 9, 9, 9, 3, 6, 8, 9, 7, 5, 6, 9, 10, 2, 5, 6, 10, 4, 3, 7, 8, 3, 10, 5, 10, 10, 10, 1, 1, 2, 3, 4, 5, 7, 1, 2, 3, 5, 6, 7, 8, 9, 2, 2, 5, 5, 8, 1, 2, 3, 6, 7, 8, 7, 1, 4, 5, 4, 1, 2, 3, 6, 7, 3, 4, 2, 2, 8, 8, 7, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 1, 3, 5, 6, 8, 1, 3, 4, 6, 7, 1, 8, 7, 5, 1, 2, 3, 4, 7, 8, 9, 1, 2, 3, 4, 5, 7, 9, 9, 3, 1, 4, 5, 3, 6, 8, 3, 9, 2, 1, 2, 3, 5, 6, 7, 8, 1, 3, 5, 6, 7, 8, 9, 4, 4, 1, 2, 3, 4, 5, 7, 8, 9, 2, 5, 1, 3, 4, 5, 6, 8, 1, 3, 5, 8, 1, 7, 2, 4, 9, 6, 10, 10, 1, 2, 5, 7, 10, 4, 3, 9, 3, 7, 2, 3, 4, 9, 6, 2, 8, 1, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 5, 7, 1, 2, 3, 4, 5, 10, 4, 5, 6, 8, 4, 6, 2, 8, 10, 2, 3, 4, 5, 6, 7, 8, 6, 3, 3, 7, 4, 2, 3, 10, 6, 4, 2, 6, 7, 8, 1, 5, 6, 5, 5, 7, 7, 2, 3, 8, 10, 2, 3, 4, 7, 8, 10, 2, 1, 2, 5, 6, 8, 9, 1, 2, 4, 6, 7, 8, 2, 7, 9, 1, 7, 10, 4, 1, 8, 9, 2, 1, 2, 1, 2, 4, 5, 2, 1, 5, 2, 4, 6, 8, 5, 8, 10, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 6, 6, 5, 1, 3, 8, 9, 8, 10, 1, 5, 6, 1, 2, 5, 6, 7, 9, 1, 2, 3, 5, 7, 8, 3, 5, 6, 8, 8, 4, 8, 1, 2, 3, 4, 5, 6, 9, 5, 8, 10, 3, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 1, 2, 3, 5, 6, 7, 8, 2, 1, 2, 3, 4, 5, 7, 8, 10, 9, 1, 2, 4, 5, 6, 8, 10, 1, 1, 2, 3, 4, 5, 6, 7, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 2, 3, 7, 1, 3, 4, 5, 6, 7, 8, 3, 4, 8, 1, 2, 3, 5, 7, 1, 5, 6, 2, 4, 4, 1, 5, 7, 9, 1, 4, 6, 9, 3, 5, 9, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 2, 4, 8, 9, 3, 4, 6, 5, 1, 2, 3, 4, 5, 2, 3, 7, 1, 2, 5, 6, 8, 9, 10, 3, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 2, 10, 1, 7, 2, 5, 9, 3, 8, 1, 7, 1, 2, 3, 4, 5, 7, 8, 10, 1, 2, 3, 5, 7, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 9, 1, 7, 3, 3, 5, 9, 5, 6, 3, 8, 7, 7, 1, 2, 4, 6, 1, 4, 5, 7, 10, 6, 2, 4, 7, 10, 2, 3, 4, 10, 3, 7, 10, 5, 3, 3, 1, 1, 1, 4, 5, 6, 1, 2, 3, 4, 5, 7, 10, 1, 5, 7, 9, 10, 4, 9, 10, 4, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 8, 6, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 8, 8, 6, 3, 6, 2, 5, 1, 4, 5, 2, 3, 4, 8, 6, 7, 3, 1, 2, 3, 4, 7, 6, 10, 5, 7, 1, 2, 3, 5, 7, 1, 10, 8, 4, 2, 1, 2, 3, 4, 5, 10, 1, 2, 5, 6, 8, 9, 1, 3, 8, 1, 2, 3, 4, 7, 9, 6, 7, 3, 6, 7, 10, 9, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, 8, 1, 2, 6, 8, 10, 1, 2, 3, 4, 5, 6, 8, 9, 2, 5, 9, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 8, 8, 10, 1, 2, 4, 5, 6, 9, 2, 3, 4, 5, 6, 4, 6, 7, 3, 5, 2, 1, 2, 4, 5, 6, 9, 1, 1, 2, 5, 9, 10, 1, 2, 3, 5, 7, 8, 9, 6, 9, 9, 5, 6, 5, 6, 9, 1, 3, 4, 7, 9, 6, 1, 2, 5, 3, 9, 4, 1, 2, 4, 5, 7, 6, 4, 5, 6, 8, 2, 3, 3, 10, 9, 9, 9, 6, 9, 9, 8, 7, 4, 1, 4, 3, 4, 7, 6, 1, 3, 4, 9, 3, 4, 3, 1, 2, 3, 4, 5, 6, 7, 8, 2, 6, 8, 1, 1, 6, 9, 1, 2, 5, 7, 8, 1, 2, 4, 5, 7, 8, 9, 1, 2, 3, 4, 6, 8, 10, 1, 2, 3, 6, 7, 9, 1, 3, 4, 5, 6, 7, 8, 2, 4, 9, 2, 1, 2, 5, 6, 7, 10, 9, 9, 5, 1, 4, 5, 7, 9, 8, 8, 1, 1, 2, 5, 6, 7, 8, 6, 1, 2, 3, 4, 6, 7, 10, 6, 10, 3, 9, 3, 4, 6, 9, 1, 2, 9, 5, 2, 3, 5, 1, 2, 4, 5, 1, 2, 7, 9, 10, 2, 3, 4, 5, 7, 9, 10, 2, 10, 2, 10, 2, 2, 2, 3, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 1, 2, 4, 5, 7, 3, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 6, 3, 4, 1, 7, 9, 1, 7, 2, 3, 10, 1, 3, 6, 5, 8, 1, 5, 3, 6, 9, 1, 2, 3, 5, 6, 7, 7, 3, 8, 5, 3, 10, 1, 2, 3, 1, 2, 3, 4, 5, 7, 9, 7, 3, 3, 1, 2, 3, 4, 5, 6, 7, 9, 1, 4, 5, 6, 7, 9, 2, 6, 1, 2, 3, 4, 6, 8, 3, 6, 8, 1, 2, 3, 4, 6, 7, 10, 4, 1, 2, 3, 5, 6, 7, 8, 9, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 4, 5, 6, 7, 3, 1, 5, 10], \"Freq\": [0.9975304470837719, 0.9864272395147972, 0.42916126304651736, 0.2461366067472673, 0.08204553558242243, 0.23351421665766384, 0.9589393672457999, 0.024588188903738456, 0.12368646657411923, 0.8658052660188347, 0.7251938737108272, 0.2510286485922094, 0.022313657652640838, 0.7810918313954144, 0.1680830523255955, 0.04943619186046926, 0.019638127674400745, 0.9622682560456365, 0.9924358418082144, 0.9111198018843056, 0.9915333780439343, 0.9611409825882667, 0.08039746193313675, 0.1473953468774174, 0.7637758883647993, 0.6208527492274799, 0.20177714349893094, 0.0429821134080563, 0.05253369416540214, 0.07999448884277144, 0.3108704607880823, 0.2594018414523071, 0.19177999389422704, 0.1569396977284715, 0.017895243030592613, 0.017103418117734533, 0.0424418153291931, 0.002692204703717473, 0.0006334599302864641, 0.00015836498257161603, 0.9881735384376267, 0.17986929561035134, 0.001993011585710264, 0.158444421063966, 0.1788727898174962, 0.1380160523104358, 0.007972046342841056, 0.009466805032123755, 0.31190631316365636, 0.013452828203544282, 0.09193994862444602, 0.41790885738384553, 0.07020868804048605, 0.01671635429535382, 0.19223807439656895, 0.16716354295353822, 0.018387989724889205, 0.010029812577212293, 0.01671635429535382, 0.10816238301578106, 0.10816238301578106, 0.3954687129014495, 0.09464208513880842, 0.0405608936309179, 0.006760148938486316, 0.18252402133913054, 0.060841340446376846, 0.07378915142669581, 0.03279517841186481, 0.770686692678823, 0.024596383808898604, 0.05739156222076341, 0.04099397301483101, 0.9554782421141723, 0.9650640717451666, 0.9855175713490898, 0.9497563857422308, 0.26543458991787305, 0.10730334486041677, 0.028237722331688624, 0.09036071146140359, 0.44615601284068024, 0.06212298912971497, 0.9674515576595906, 0.9802870965538504, 0.995125684923092, 0.2976101506242789, 0.36271237107334, 0.0031001057356695723, 0.02790095162102615, 0.26040888179624405, 0.046501586035043584, 0.9613002791465126, 0.10208757307335713, 0.22737686729974996, 0.027842065383642853, 0.5011571769055714, 0.06032447499789285, 0.08352619615092856, 0.004640344230607142, 0.9901413892780151, 0.9698656007765202, 0.9941427534815528, 0.044973184167505544, 0.8769770912663581, 0.06745977625125832, 0.9788004197068974, 0.9788004197068974, 0.929141228238938, 0.033183615294247785, 0.9668573304247305, 0.9525157920634673, 0.04246885697098262, 0.8656987239348818, 0.006183562313820584, 0.12367124627641168, 0.95673664880576, 0.5524125260046759, 0.20982192511336403, 0.13751041419858884, 0.049788253416730446, 0.0486028188115702, 0.0011854346051602487, 0.9922818072423817, 0.052127949843386555, 0.46335955416343605, 0.28766905654313324, 0.06371193869747246, 0.005791994427042951, 0.1023252348777588, 0.027029307326200437, 0.15993509488883131, 0.8174460405429157, 0.9577785601684428, 0.8504020279423545, 0.11649342848525404, 0.02329868569705081, 0.1470488814891499, 0.8271499583764683, 0.10074505625424528, 0.0783572659755241, 0.6212611802345126, 0.1623114795207285, 0.011193895139360586, 0.005596947569680293, 0.01679084270904088, 0.9336476364612457, 0.989318945013982, 0.9656373603789902, 0.02611457828083425, 0.02611457828083425, 0.9140102398291988, 0.9595610555527019, 0.9649288187144148, 0.9825564273633164, 0.9836377060281818, 0.9510845291351064, 0.9575475613504518, 0.9702137295358582, 0.9545658656390983, 0.21099948568224258, 0.07719493378618632, 0.32421872190198253, 0.0823412627052654, 0.2470237881157962, 0.05660961810986996, 0.9278825761677014, 0.9919994586643084, 0.08881156359169606, 0.42791026094180834, 0.4198364824334723, 0.06055333881252004, 0.9858784994543803, 0.9746308851323817, 0.9856181773566496, 0.9669320439135076, 0.9194248543558076, 0.05629131761362087, 0.9498078690919748, 0.024354047925435253, 0.2186388810992983, 0.7692849520160495, 0.9849743065712708, 0.9638815855626024, 0.9476008900800477, 0.018462135949214104, 0.03692427189842821, 0.9415689334099193, 0.017601915693448475, 0.04400478923362119, 0.15841724124103626, 0.7744842905117328, 0.7273396553717261, 0.012433156502080787, 0.0932486737656059, 0.06838236076144433, 0.0870320955145655, 0.012433156502080787, 0.9608429935864585, 0.9879390417759564, 0.9946513770673974, 0.9192859343479857, 0.9225226610518105, 0.9811658035340368, 0.9831828267311895, 0.9669127945181384, 0.640421255323061, 0.31245466090156265, 0.04431981005695924, 0.9733049468031347, 0.9287646939990977, 0.9920333146029369, 0.9657443954845876, 0.22516790927039412, 0.7317957051287809, 0.04221898298819889, 0.23883730114094656, 0.7532561035983699, 0.5088454204038566, 0.0007792426039875292, 0.16987488766928138, 0.0007792426039875292, 0.2742933966036103, 0.04597531363526423, 0.9706522340077997, 0.31543411948530886, 0.6810509397978259, 0.9498199894577447, 0.003528955200867265, 0.3105480576763193, 0.00705791040173453, 0.017644776004336324, 0.01411582080346906, 0.08469492482081437, 0.10939761122688521, 0.4552352209118772, 0.9573330273804932, 0.018410250526547947, 0.9866739304003215, 0.019624063874633176, 0.4709775329911962, 0.5036843061155848, 0.9606077747968861, 0.9587525561200055, 0.9601915145658331, 0.9486252593969723, 0.03794501037587889, 0.9286229474226886, 0.010912447782360113, 0.06001846280298062, 0.16914294062658175, 0.09821203004124102, 0.5947284041386262, 0.06001846280298062, 0.9455445405261972, 0.17886274587506676, 0.20940028785373668, 0.21376279385068955, 0.3926255397257563, 0.9604589584511327, 0.20630561472256592, 0.48137976768598717, 0.07163389400089094, 0.034384269120427656, 0.17478670136217392, 0.02865355760035638, 0.12207403312234556, 0.8256059608537581, 0.03533722011436319, 0.006424949111702398, 0.009637423667553597, 0.06549575889354535, 0.33566576432941997, 0.1883003068189429, 0.4011615232229653, 0.07965416273374738, 0.6945842990382772, 0.0015930832546749477, 0.047792497640248434, 0.046199414385573484, 0.022303165565449267, 0.10514349480854655, 0.0031861665093498954, 0.9541540317613209, 0.9345080824621298, 0.9489496100713378, 0.8559941746193194, 0.9870602569769539, 0.9529575289679354, 0.04529903746854622, 0.9584638592375182, 0.9735830980424445, 0.759199459189518, 0.08076589991377851, 0.010768786655170469, 0.14537861984480133, 0.9621672246799322, 0.9481158165982435, 0.10616479487553425, 0.8139300940457626, 0.007077652991702283, 0.04954357094191598, 0.02123295897510685, 0.2221352473059804, 0.33660985327961446, 0.11856298475840671, 0.03883959845534013, 0.07154662873352129, 0.04088378784772645, 0.025893065636893416, 0.11311181304537651, 0.029981444421666063, 0.0020441893923863223, 0.979823876258065, 0.015682262305332154, 0.35197966507523276, 0.5262270240233679, 0.04181936614755241, 0.06272904922132862, 0.9529936686162626, 0.9840727761366441, 0.09550254509701962, 0.7544701062664549, 0.1241533086261255, 0.019100509019403925, 0.9764542647412787, 0.01668357999680669, 0.917596899824368, 0.050050739990420076, 0.9369761152369632, 0.24223534483115539, 0.14534120689869323, 0.04844706896623108, 0.5329177586285418, 0.9692550627586889, 0.04963409849441004, 0.36161986045927313, 0.5885185964337191, 0.0653929581415886, 0.8501084558406519, 0.956722532449256, 0.8559807309715848, 0.8559807309715848, 0.9162602488441073, 0.9599379276552724, 0.10677394472845271, 0.020020114636584884, 0.28695497645771667, 0.17350766018373565, 0.013346743091056589, 0.40040229273169764, 0.037913565428545445, 0.14443263020398264, 0.47843308755069247, 0.08846498599993936, 0.17692997199987873, 0.001805407877549783, 0.06499468359179218, 0.003610815755099566, 0.9723669359032571, 0.964932660297447, 0.942196653128963, 0.6345479674764931, 0.3512676248530587, 0.48833107207901977, 0.3171634798038995, 0.15858173990194974, 0.005034340949268246, 0.01762019332243886, 0.012585852373170614, 0.956739322613702, 0.720184825790543, 0.02250577580595447, 0.24756353386549915, 0.956624165688959, 0.32702927803926657, 0.003802666023712402, 0.25477862358873093, 0.045631992284548825, 0.368858604300103, 0.980373625792628, 0.9721462927128428, 0.97478690542687, 0.04212796696519663, 0.95665591650134, 0.973052805404921, 0.9361782366576834, 0.1810537213412158, 0.07242148853648632, 0.00965619847153151, 0.393490087714909, 0.007242148853648632, 0.05310909159342331, 0.0024140496178828774, 0.2824438052922967, 0.6950663315663198, 0.12489473145332308, 0.04887185143825686, 0.010860411430723747, 0.11946452573796121, 0.9500183131068555, 0.062133809463991496, 0.2692465076772965, 0.662760634282576, 0.9438733700282951, 0.08918233728299287, 0.20809212032698338, 0.1387280802179889, 0.009909148586999207, 0.5549123208719556, 0.11815158652075547, 0.8565990022754771, 0.9592993462258705, 0.9602274367046673, 0.010066372573476702, 0.04026549029390681, 0.2063606377562724, 0.010066372573476702, 0.04026549029390681, 0.2063606377562724, 0.48318588352688174, 0.1386666748574763, 0.012606061350679664, 0.012606061350679664, 0.012606061350679664, 0.11345455215611697, 0.11345455215611697, 0.5798788221312645, 0.975478430276642, 0.977513556255361, 0.45917018341633586, 0.17525579519707477, 0.36102693810597397, 0.5861282160544499, 0.017583846481633494, 0.3985671869170259, 0.8252713476544146, 0.17109284036737862, 0.9681871284999074, 0.03188131696510267, 0.0515005889436274, 0.5395299794094298, 0.012262044986577951, 0.0760246789167833, 0.27221739870203054, 0.014714453983893542, 0.07706100305197562, 0.7898752812827501, 0.019265250762993905, 0.044952251780319114, 0.025687001017325205, 0.006421750254331301, 0.03853050152598781, 0.9356549597503503, 0.93222396355503, 0.004542552597605614, 0.21349997208746385, 0.10902126234253474, 0.036340420780844915, 0.009085105195211229, 0.08176594675690105, 0.4269999441749277, 0.11356381494014035, 0.9473922675072718, 0.04621425695157423, 0.02242774820096312, 0.7550675227657583, 0.02242774820096312, 0.020932564987565575, 0.05681696210910656, 0.12260502349859838, 0.015400040743248177, 0.7238019149326643, 0.1386003666892336, 0.11550030557436132, 0.9708573782730098, 0.9489103006640641, 0.3860265994362843, 0.023395551480986928, 0.5848887870246732, 0.9807405361192195, 0.9605436786926638, 0.9425824054139049, 0.5877229444042225, 0.08979100539508954, 0.2571287881768473, 0.0612211400421065, 0.0040814093361404334, 0.9647125280885506, 0.9757195005491766, 0.9382816998956719, 0.2575539084433558, 0.7358683098381594, 0.05692907422119018, 0.03162726345621677, 0.12650905382486707, 0.7780306810229325, 0.9720782924684277, 0.9644630544366334, 0.9487275343010111, 0.26153796016603603, 0.6342295534026373, 0.1046151840664144, 0.30305119763304994, 0.21303599041531232, 0.18003041443547518, 0.05400912433064256, 0.022503801804434398, 0.20703497660079648, 0.01050177417540272, 0.00900152072177376, 0.08317684627030238, 0.8650392012111447, 0.04990610776218143, 0.16478030856880624, 0.056175105193911216, 0.16478030856880624, 0.007490014025854828, 0.10860520337489502, 0.5018309397322736, 0.5958708743075029, 0.07621604206258757, 0.2979354371537514, 0.02771492438639548, 0.9293373404466572, 0.060741002643572364, 0.1899680012200672, 0.7637489028643518, 0.03876897984083004, 0.05081772816303556, 0.2871201641211509, 0.002540886408151778, 0.002540886408151778, 0.007622659224455334, 0.002540886408151778, 0.6504669204868552, 0.9649984358821928, 0.9657499756514468, 0.807465794858426, 0.18030789593926017, 0.969784849933145, 0.883058826628593, 0.07569075656816512, 0.0403684035030214, 0.9916310500209674, 0.9558661894059792, 0.11850178803138435, 0.1580023840418458, 0.7110107281883061, 0.9789221205575949, 0.06590779771712404, 0.8568013703226125, 0.04393853181141603, 0.9785775016760008, 0.9574427832117252, 0.9515132196108087, 0.966144999968599, 0.8933462136580492, 0.024308740507702018, 0.0060771851269255045, 0.07292622152310606, 0.4416886833508603, 0.1115375463007223, 0.24538260186158906, 0.03390741407541958, 0.0008923003704057784, 0.1659678688954748, 0.9593746825391921, 0.2668969066666872, 0.6122929035294589, 0.07849909019608446, 0.015699818039216894, 0.015699818039216894, 0.01177486352941267, 0.2749129062410116, 0.6935302861989157, 0.024992082385546507, 0.006248020596386627, 0.9715429637783963, 0.967895430921065, 0.982422084011259, 0.0889614811458226, 0.9044417249825298, 0.9794881185051894, 0.9696028980180236, 0.923887730165731, 0.9956397129030075, 0.98092231382234, 0.9639428153558717, 0.9808106419153991, 0.9954883669102872, 0.9464790550588824, 0.04732395275294412, 0.20440248326588129, 0.7930816350716194, 0.12951803892211133, 0.8657258391109548, 0.9605642579089478, 0.013688125901256768, 0.9718569389892305, 0.6436482263277559, 0.0861387960884729, 0.22970345623592772, 0.03589116503686371, 0.9401676802001276, 0.9514327358810164, 0.9704136598685307, 0.9621315496373114, 0.02872034476529288, 0.17081801995840423, 0.281618897769261, 0.011541758105297584, 0.05078373566330937, 0.2954690074956181, 0.10387582294767825, 0.07386725187390453, 0.009233406484238066, 0.9564059069487393, 0.9935728882791262, 0.9717885229141291, 0.9606963063452695, 0.13493975424640664, 0.08587075270225877, 0.5888280185297744, 0.1840087557905545, 0.9616564056596084, 0.856024275394389, 0.11049462658716738, 0.8102939283058942, 0.05524731329358369, 0.028508577612947887, 0.3135943537424267, 0.22236690538099352, 0.06271887074848535, 0.028508577612947887, 0.3421029313553746, 0.24997006651254536, 0.12703396822768698, 0.2376764566840595, 0.02048934971414306, 0.3442210751976034, 0.02048934971414306, 0.08756872918092558, 0.00625490922720897, 0.015637273068022425, 0.889760837570476, 0.9638189292487404, 0.5364616534838715, 0.4637210902996178, 0.06457732482710386, 0.08984671280292711, 0.028077097750914725, 0.10107755190329301, 0.07300045415237828, 0.5727727941186603, 0.06738503460219533, 0.971709311965285, 0.8970988048672862, 0.09967764498525401, 0.5832970150153873, 0.023331880600615494, 0.3888646766769249, 0.2500216158184149, 0.18258157470950037, 0.06661760158319607, 0.055925887748855964, 0.08059907352041007, 0.057570766800292904, 0.192450849018122, 0.09046834782903171, 0.023028306720117162, 0.39614572915912794, 0.002866466925898176, 0.1318574785913161, 0.14045687936901063, 0.005732933851796352, 0.0011465867703592705, 0.3153113618487994, 0.005159640466616717, 0.0005732933851796353, 0.9896118058656871, 0.05962710907728578, 0.918257479790201, 0.011925421815457155, 0.12260805481737388, 0.020749055430632504, 0.1848552211092714, 0.08110994395610888, 0.04149811086126501, 0.399890886481281, 0.056588332992634105, 0.04527066639410728, 0.03395299979558046, 0.013203944364947958, 0.11155636583114112, 0.8738581990106055, 0.19621068892349627, 0.1464258872563405, 0.2049962421588767, 0.09956960333431154, 0.058570354902536204, 0.24599549059065204, 0.04392776617690215, 0.957740037180109, 0.1685663076085485, 0.1659729797991862, 0.12966639046811423, 0.031119933712347413, 0.062239867424694825, 0.04667990056852112, 0.25414612531750386, 0.13744637389620107, 0.9425615345051733, 0.20136479780950806, 0.08457321507999338, 0.07651862316761306, 0.2939926048018818, 0.18928290994093758, 0.044300255518091775, 0.1047096948609442, 0.9755071426220174, 0.028843604500350808, 0.3007975897893727, 0.10095261575122783, 0.002060257464310772, 0.08241029857243089, 0.3914489182190467, 0.09271158589398475, 0.1804637756774351, 0.15505551845209226, 0.13941966785188128, 0.14332863050193403, 0.003908962650052746, 0.013681369275184612, 0.3329133190294922, 0.030620207425413177, 0.15904991470698074, 0.5259491497696749, 0.0018073853943975083, 0.07048803038150282, 0.027110780915962623, 0.21146409114450848, 0.0018073853943975083, 0.8486939207964737, 0.02390687100835137, 0.11953435504175686, 0.372371657810284, 0.067703937783688, 0.067703937783688, 0.02417997777988857, 0.014507986667933142, 0.4352396000379943, 0.019343982223910858, 0.02753307422589113, 0.8810583752285162, 0.05506614845178226, 0.3453948464029343, 0.3191367586646995, 0.1676477909441143, 0.15754852642940861, 0.008079411611764543, 0.10383264768481507, 0.8763475464598393, 0.016613223629570413, 0.9814674518650932, 0.9677365310781942, 0.9781617610127639, 0.29766749910942153, 0.01793177705478443, 0.10759066232870658, 0.5774032211640586, 0.9476883059317257, 0.037164247291440224, 0.1610971703200521, 0.8284997330745537, 0.1872804034432117, 0.23049895808395288, 0.5618412103296351, 0.9205235682396041, 0.9443169176861566, 0.3827901092770605, 0.4189023837371605, 0.049009515338707205, 0.05623197023072721, 0.06758154220390152, 0.0005158896351442863, 0.009286013432597153, 0.003095337810865718, 0.012381351243462872, 0.956467072230049, 0.9810484543907769, 0.9959560534231103, 0.053237262353009115, 0.9316520911776595, 0.9918409728621432, 0.8560702011954286, 0.14316532374940616, 0.9945398403700505, 0.16018063687280473, 0.31565007854346816, 0.014133585606423946, 0.4004515921820118, 0.10364629444710895, 0.11765220521992088, 0.808858910886956, 0.06863045304495385, 0.1323199462365976, 0.04961997983872409, 0.3363131966846855, 0.022053324372766265, 0.292206547939153, 0.14334660842298072, 0.02756665546595783, 0.060589636837705164, 0.8936971433561512, 0.030294818418852582, 0.2072292874631656, 0.11270364756768655, 0.26418704483608246, 0.13815285830877705, 0.15390713162469025, 0.009694937425177338, 0.027872945097384846, 0.050898421482181024, 0.037567882522562185, 0.9540565635467736, 0.04472140141625502, 0.9604564676033978, 0.9293665732394009, 0.15330421500705402, 0.8322228814668646, 0.9052230697585485, 0.07671381947106344, 0.9594568267724431, 0.41558123675625613, 0.5800821429722741, 0.9532762725240282, 0.017653264306000525, 0.22885447374899465, 0.5315796953536774, 0.08400987011039045, 0.02896892072772084, 0.06807696371014398, 0.017381352436632504, 0.0007242230181930211, 0.04055648901880918, 0.12720123394061933, 0.04770046272773225, 0.26235254500252736, 0.015900154242577416, 0.1828517737896403, 0.3736536247005693, 0.9854726748286604, 0.17196770181063922, 0.2708871763034848, 0.0707654702141126, 0.03956778979713822, 0.1460964546355873, 0.17425045891432026, 0.03043676138241402, 0.014457461656646658, 0.0814183366979575, 0.9599109201185347, 0.9647934546237807, 0.9930082113256358, 0.9613410330594279, 0.9723497202513331, 0.9889363491039874, 0.9923987802943456, 0.9133486918604767, 0.058925722055514634, 0.10050063749632754, 0.8844056099676824, 0.09464661575965364, 0.8754811957767962, 0.956244997757769, 0.9694990071745798, 0.005520771242742601, 0.24291393468067446, 0.5741602092452306, 0.17114390852502065, 0.04258786102875666, 0.6956017301363588, 0.1419595367625222, 0.08517572205751332, 0.028391907352504443, 0.9457711481790371, 0.007819066517424268, 0.29712452766212216, 0.25802919507500083, 0.437867724975759, 0.005717649131352178, 0.02858824565676089, 0.12578828088974792, 0.8404944223087703, 0.4945115295357411, 0.18422978551331534, 0.3199780485231266, 0.9587184217318159, 0.9862710106400897, 0.9598567296586565, 0.9678853612409948, 0.990843955049829, 0.8079199667154024, 0.043908693843228395, 0.13172608152968518, 0.008781738768645679, 0.21345419844954458, 0.4740668826030583, 0.007446076690100392, 0.05088152404901935, 0.12658330373170668, 0.05832760073911974, 0.06825570299258693, 0.27411344994082193, 0.5824910811242466, 0.13705672497041096, 0.0578956488059771, 0.9263303808956336, 0.11292615113002721, 0.22585230226005443, 0.6493253689976565, 0.9519239657788764, 0.9283647665737731, 0.04317975658482665, 0.0669740603355633, 0.41300670540264034, 0.03125456148992954, 0.03125456148992954, 0.3371027703556686, 0.044649373557042196, 0.00446493735570422, 0.046881842234894305, 0.022324686778521098, 0.9731999168811276, 0.9158541109205773, 0.9438894899417917, 0.5956828657699221, 0.29223939966455426, 0.04294892135645206, 0.04948462678025999, 0.0009336722034011318, 0.015872427457819242, 0.0018673444068022637, 0.0018673444068022637, 0.2880294203781548, 0.0682174943000893, 0.0682174943000893, 0.523000789634018, 0.05305805112229168, 0.9789754653340337, 0.952515257056788, 0.9872823505957782, 0.9708239506871607, 0.9627168809595217, 0.09884514366550184, 0.8896062929895165, 0.9709463169955377, 0.9678433277621613, 0.9633490869313085, 0.10201145311053135, 0.2350698702112244, 0.31046963990161713, 0.345951884461802, 0.9838537624946773, 0.9939746223275123, 0.9860014592977493, 0.2705641613883193, 0.11996712816274534, 0.025524920885690496, 0.4466861154995837, 0.1327295886055906, 0.9872298897709009, 0.9257832803109618, 0.936368326841602, 0.9604361890398324, 0.6051710143859572, 0.32617235073808826, 0.001347819631149125, 0.0053912785245965, 0.060651883401710634, 0.29775559879125046, 0.6871283049028857, 0.9660168696258238, 0.9805294263495782, 0.9758487619277529, 0.3481526796485476, 0.2852605826797777, 0.13926107185941905, 0.1482456571406719, 0.004492292640626421, 0.07412282857033595, 0.6266405310765903, 0.18317184754546487, 0.0867656119952202, 0.0144609353325367, 0.004820311777512233, 0.07712498844019573, 0.002460759671166942, 0.9572355120839405, 0.039372154738671075, 0.20115752939505083, 0.05028938234876271, 0.28162054115307117, 0.003352625489917514, 0.18104177645554573, 0.2749152901732361, 0.9718486152663385, 0.9587983107552649, 0.12690428767359266, 0.6052358335202112, 0.2635704436297694, 0.876004304478905, 0.9564008855478469, 0.9796111130190844, 0.18224394572589725, 0.027664573920832602, 0.08128251100450816, 0.20448968557976266, 0.20819730888874022, 0.0014260089649913714, 0.0775748876955306, 0.18309955110489207, 0.03422421515979291, 0.9870664740675367, 0.9423652564895734, 0.9613277772687151, 0.04346578045123189, 0.391192024061087, 0.1159087478699517, 0.0724429674187198, 0.376703430577343, 0.38882360003132, 0.14117999763041977, 0.07869049048252905, 0.011572130953313095, 0.17821081668102168, 0.057860654766565475, 0.1319222928677693, 0.013886557143975714, 0.24434744006900705, 0.7330423202070211, 0.013574857781611502, 0.202643760121593, 0.49184390019462015, 0.1456183803888834, 0.06109862114218884, 0.009164793171328326, 0.03767748303768312, 0.0509155176184907, 0.15441508880387794, 0.1584085824798403, 0.09584384822309666, 0.5111671905231822, 0.04392843043558597, 0.033279113966353004, 0.0013311645586541203, 0.06761082685358184, 0.005634235571131821, 0.04507388456905457, 0.5052031228781533, 0.08826969061439853, 0.039439648997922745, 0.13334357518345308, 0.052586198663896994, 0.06009851275873942, 0.9576115138527985, 0.9542264108499698, 0.8560336129327002, 0.7126454329940873, 0.2523465795520047, 0.016355796822815117, 0.014019254419555816, 0.7964037719381561, 0.19306758107591662, 0.1607739724270323, 0.08931887357057351, 0.014291019771291762, 0.4501671227956905, 0.2858203954258352, 0.005585023128132986, 0.05026520815319687, 0.9382838855263416, 0.9891745131658365, 0.974951403944927, 0.9727750572987495, 0.32770222321988746, 0.017555476243922542, 0.055592341439088054, 0.5500715889762396, 0.014629563536602119, 0.032185039780524664, 0.9725968962836461, 0.022191220411213942, 0.8062810082741065, 0.16273561634890224, 0.9748376056625486, 0.856263805014722, 0.12367094899662347, 0.31030165384607344, 0.26083327424742403, 0.04946837959864939, 0.0629597558528265, 0.11017957274244637, 0.08094825752506264, 0.9726920279459366, 0.9762644580443002, 0.9880991547296109, 0.8120766737927739, 0.17225868838028538, 0.24896738667278515, 0.6490935438254756, 0.09780861619287988, 0.08672426755861098, 0.2838248756463632, 0.03942012161755045, 0.2207526810582825, 0.36266511888146413, 0.9787630620990382, 0.06920674273746884, 0.3064870035516477, 0.6228606846372196, 0.9427779935680484, 0.052376555198224914, 0.9692829406436632, 0.36419812864937384, 0.0051295511077376595, 0.4001049864035374, 0.07181371550832723, 0.15388653323212978, 0.9543268513523527, 0.8252925170594572, 0.06228622770260054, 0.09342934155390081, 0.9704828935976838, 0.9599907132341178, 0.9778052347953069, 0.9729683435290928, 0.9732921537156414, 0.9640736544635382, 0.9482442497393246, 0.9387401113346885, 0.9478350405723762, 0.965004193412477, 0.9554456677177972, 0.9706283837224322, 0.9276541644745028, 0.9592428923842269, 0.9806631337594819, 0.9351524654049577, 0.9844741014750988, 0.9899413286022696, 0.9661958382135544, 0.9443062402211367, 0.9702234502766218, 0.24727152083881856, 0.7153211852837251, 0.02649337723273056, 0.0876914584840766, 0.9097988817722947, 0.9655363627099062, 0.14156716108565984, 0.3333678309436506, 0.12330043062299405, 0.0045666826156664465, 0.018266730462665786, 0.3059677352496519, 0.0685002392349967, 0.0045666826156664465, 0.17251644060028878, 0.10350986436017327, 0.7245690505212129, 0.9737804855342478, 0.08077131856496723, 0.9086773338558813, 0.9841508465385065, 0.5265131650911231, 0.023932416595051048, 0.27671856688027774, 0.08675501015706005, 0.08675501015706005, 0.2350719919149045, 0.17096144866538507, 0.017096144866538508, 0.1111249416325003, 0.4017594043636549, 0.025644217299807762, 0.04274036216634627, 0.20151115676524142, 0.030664741246884564, 0.013142031962950527, 0.004380677320983509, 0.41616434549343334, 0.31102808978982915, 0.021903386604917544, 0.045917596721699754, 0.045917596721699754, 0.24489384918239868, 0.558664093447347, 0.06887639508254963, 0.038264663934749796, 0.232681153639242, 0.20206521237092068, 0.19900361824408855, 0.0581702884098105, 0.19594202411725642, 0.11021738856595673, 0.0030615941268321316, 0.9268786710188937, 0.052464830435031716, 0.017488276811677236, 0.9902952952723214, 0.21407138697260014, 0.005524422889615488, 0.04833870028413552, 0.001381105722403872, 0.6988394955363592, 0.03176543161528905, 0.946510621492043, 0.9577843298584947, 0.9782690201769046, 0.3160977009422899, 0.1159024903455063, 0.11379517233922437, 0.4509660533443336, 0.002107318006281933, 0.9703916427567687, 0.9504944659912106, 0.9913033393835464, 0.08113861473761969, 0.44839760776052984, 0.04270453407243141, 0.09822042836659226, 0.021352267036215705, 0.30320219191426306, 0.9736057596994437, 0.238708775045489, 0.13588037964127836, 0.3048127435196244, 0.03672442693007523, 0.003672442693007523, 0.26441587389654164, 0.01101732807902257, 0.9662866023686212, 0.9469758341185233, 0.018465367225865153, 0.9601990957449881, 0.32211376033562156, 0.010066055010488174, 0.23151926524122798, 0.43284036545099147, 0.9765658850121711, 0.2549504343325833, 0.7365234769607961, 0.944347315343869, 0.8780887914983332, 0.10076428754898906, 0.9701739301966092, 0.3613449308442584, 0.1266026035074774, 0.013187771198695563, 0.49849775131069224, 0.24314170514709924, 0.40118381349271376, 0.328241301948584, 0.012157085257354962, 0.006078542628677481, 0.11710192558372835, 0.19421782779740313, 0.11424578105729595, 0.43413396801772464, 0.02284915621145919, 0.03427373431718878, 0.08568433579297197, 0.17470445310568056, 0.8007287434010358, 0.40918403630667244, 0.5650636691854047, 0.9683264232584361, 0.9728030990203382, 0.08163487669404354, 0.048980926016426125, 0.8000217916016267, 0.06530790135523483, 0.8559326245283004, 0.4474399364502584, 0.2425321259672164, 0.059041233529012103, 0.04457034295817581, 0.08045815157384983, 0.029520616764506052, 0.041097329221175094, 0.03762431548417438, 0.017943904307837013, 0.315756320055244, 0.059304486507837706, 0.18913322724121215, 0.012822591677370316, 0.1634880438864715, 0.09777226153994865, 0.07853837402389317, 0.030453655233754498, 0.05129036670948126, 0.9590132868033463, 0.9821321641128785, 0.01492365851652191, 0.2104235850829589, 0.03133968288469601, 0.15072895101687128, 0.5909768772542676, 0.9828408239973889, 0.014720588635320638, 0.4011360403124874, 0.12144485624139527, 0.0036801471588301595, 0.0736029431766032, 0.040481618747131756, 0.2649705954357715, 0.0036801471588301595, 0.08096323749426351, 0.9543403122446257, 0.9857913896566891, 0.08409754765493567, 0.8970405083193138, 0.9761422079539437, 0.9578318534827991, 0.038313274139311966, 0.025009767747191953, 0.9753809421404862, 0.8051663657513571, 0.008215983323993441, 0.17253564980386224, 0.15456696826882813, 0.21639375557635937, 0.6182678730753125, 0.8874992637990585, 0.093420975136743, 0.20797030867522193, 0.7798886575320823, 0.19599794248080174, 0.4899948562020044, 0.3094704354960028, 0.05932898542214232, 0.382671955972818, 0.2580810865863191, 0.26104753585742624, 0.02373159416885693, 0.011865797084428465, 0.9528109533283604, 0.9093115303622492, 0.08747807127535562, 0.9689963390824576, 0.9799334174096743, 0.9564218218732248, 0.15084112200475394, 0.08380062333597442, 0.7625856723573672, 0.11859854770874131, 0.1680146092540502, 0.02964963692718533, 0.009883212309061777, 0.009883212309061777, 0.22731388310842085, 0.4249781292896564, 0.9833829622302513, 0.9692425452073251, 0.9862305983085105, 0.3429199954564347, 0.08945739011906993, 0.1118217376488374, 0.041001303804573716, 0.007454782509922494, 0.1192765201587599, 0.27582695286713227, 0.007454782509922494, 0.8995668184773478, 0.07269226815978568, 0.01817306703994642, 0.012717055523676368, 0.9410621087520512, 0.038151166571029106, 0.9821303008029846, 0.9449210610605943, 0.005460707945469323, 0.0027303539727346615, 0.7808812362021131, 0.11740522082759045, 0.010921415890938646, 0.08191061918203985, 0.13740223495381435, 0.004041242204523951, 0.8567433473590778, 0.4407649209893233, 0.5053883492546752, 0.029826197660931652, 0.0016570109811628695, 0.0016570109811628695, 0.014913098830465826, 0.006628043924651478, 0.9668789619977084, 0.10184260364142157, 0.4572082844327649, 0.0021668639072642886, 0.11701065099227159, 0.1538473374157645, 0.03466982251622862, 0.12784497052859303, 0.004333727814528577, 0.9782886505314423, 0.9633552999341611, 0.17577602727027974, 0.3529696031475779, 0.12049163159656273, 0.014884260373693043, 0.06378968731582733, 0.23176919724750597, 0.034729940871950436, 0.006378968731582733, 0.20642893808502158, 0.6380530813537031, 0.0750650683945533, 0.0750650683945533, 0.9861666573635895, 0.3325836550689758, 0.6651673101379516, 0.8561043335395724], \"Term\": [\"abbas\", \"abd\", \"abdullah\", \"abdullah\", \"abdullah\", \"abdullah\", \"abi\", \"abi\", \"abide\", \"abide\", \"abraham\", \"abraham\", \"abraham\", \"abu\", \"abu\", \"abu\", \"abundance\", \"abundance\", \"abundant\", \"abuse\", \"acts\", \"admonished\", \"age\", \"age\", \"age\", \"al\", \"al\", \"al\", \"al\", \"al\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"allah\", \"alms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"angels\", \"angels\", \"angels\", \"angels\", \"angels\", \"angels\", \"angels\", \"angels\", \"appointed\", \"appointed\", \"appointed\", \"appointed\", \"appointed\", \"appointed\", \"arabic\", \"arouse\", \"art\", \"ascent\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"astonishment\", \"atom\", \"authority\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"ayb\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"badr\", \"baked\", \"banu\", \"beasts\", \"beasts\", \"beasts\", \"begat\", \"begetter\", \"beginning\", \"beginning\", \"begotten\", \"beings\", \"beings\", \"belief\", \"belief\", \"belief\", \"belieth\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believer\", \"believers\", \"believers\", \"believers\", \"believers\", \"believers\", \"believers\", \"believers\", \"belong\", \"belong\", \"belongeth\", \"beneath\", \"beneath\", \"beneath\", \"bestow\", \"bestow\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"bewitched\", \"bint\", \"black\", \"blind\", \"blind\", \"blind\", \"body\", \"bones\", \"bosom\", \"bounties\", \"bow\", \"bread\", \"bridge\", \"brightness\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"building\", \"calamity\", \"came\", \"came\", \"came\", \"came\", \"camel\", \"camels\", \"cattle\", \"charity\", \"child\", \"child\", \"choose\", \"choose\", \"christians\", \"christians\", \"city\", \"clarification\", \"clouds\", \"comfort\", \"comfort\", \"comfort\", \"coming\", \"coming\", \"coming\", \"coming\", \"commands\", \"commands\", \"commands\", \"commands\", \"commands\", \"commands\", \"conjunction\", \"contains\", \"convey\", \"cooking\", \"couches\", \"counted\", \"courses\", \"create\", \"created\", \"created\", \"created\", \"createth\", \"creating\", \"creation\", \"crops\", \"darkness\", \"darkness\", \"darkness\", \"dawn\", \"dawn\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"daybreak\", \"dead\", \"dead\", \"dealt\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"declare\", \"declare\", \"deed\", \"denied\", \"denied\", \"denied\", \"denies\", \"descend\", \"describe\", \"description\", \"description\", \"destitute\", \"destroyed\", \"destroyed\", \"destroyed\", \"destroyed\", \"destroyed\", \"destroyed\", \"didst\", \"die\", \"die\", \"die\", \"die\", \"dilate\", \"disbelief\", \"disbelief\", \"disbelief\", \"disbelief\", \"disbelief\", \"disbelief\", \"disbelieve\", \"disbelieve\", \"disbelieve\", \"disbelieve\", \"disbelieve\", \"disbeliever\", \"disbeliever\", \"disbeliever\", \"disbeliever\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelievers\", \"disbelieveth\", \"discourse\", \"disobedient\", \"dissuadeth\", \"distracted\", \"divine\", \"divine\", \"divinity\", \"dominion\", \"doom\", \"doom\", \"doom\", \"doom\", \"drop\", \"drowning\", \"due\", \"due\", \"due\", \"due\", \"due\", \"e\", \"e\", \"e\", \"e\", \"e\", \"e\", \"e\", \"e\", \"e\", \"e\", \"ears\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"earthquake\", \"easy\", \"eat\", \"eat\", \"eat\", \"eat\", \"eats\", \"egypt\", \"egypt\", \"egypt\", \"enjoins\", \"enmity\", \"enmity\", \"enmity\", \"enmity\", \"enshrouds\", \"enter\", \"enter\", \"enter\", \"enters\", \"enters\", \"entire\", \"envier\", \"envieth\", \"erring\", \"event\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"exhort\", \"expedition\", \"extent\", \"eyes\", \"eyes\", \"faith\", \"faith\", \"faith\", \"faith\", \"faith\", \"faith\", \"falaq\", \"father\", \"father\", \"father\", \"favours\", \"fear\", \"fear\", \"fear\", \"fear\", \"fear\", \"fig\", \"final\", \"finish\", \"fire\", \"fire\", \"flaming\", \"flee\", \"folk\", \"folk\", \"folk\", \"folk\", \"folk\", \"folk\", \"folk\", \"folk\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follows\", \"food\", \"food\", \"food\", \"forelock\", \"forgiveness\", \"forgiveness\", \"forgiveness\", \"forgiveness\", \"forgiveness\", \"former\", \"former\", \"forms\", \"forsaken\", \"forth\", \"forth\", \"forth\", \"forth\", \"forth\", \"forth\", \"forth\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"fruit\", \"fulfil\", \"gabriel\", \"gabriel\", \"gabriel\", \"garden\", \"garden\", \"garden\", \"gardens\", \"gardens\", \"gehenna\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"giving\", \"giving\", \"giving\", \"giving\", \"giving\", \"giving\", \"giving\", \"glorified\", \"glory\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goeth\", \"goeth\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goodness\", \"goodness\", \"goodness\", \"goodness\", \"gospel\", \"grateful\", \"graves\", \"graves\", \"graves\", \"green\", \"greet\", \"greeting\", \"guidance\", \"guidance\", \"guidance\", \"guidance\", \"guidance\", \"ha\", \"habitations\", \"halter\", \"hamstrung\", \"hamstrung\", \"hand\", \"hand\", \"hand\", \"hand\", \"haply\", \"happy\", \"hardships\", \"hast\", \"hast\", \"hast\", \"hath\", \"hath\", \"hath\", \"hath\", \"hath\", \"hath\", \"hath\", \"hath\", \"heart\", \"heart\", \"heart\", \"hearts\", \"hearts\", \"hearts\", \"hearts\", \"hearts\", \"hearts\", \"heaven\", \"heaven\", \"heaven\", \"heaven\", \"heavens\", \"heavens\", \"hell\", \"hell\", \"hell\", \"hereafter\", \"hereafter\", \"hereafter\", \"hereafter\", \"hereafter\", \"hereafter\", \"hereafter\", \"hills\", \"historic\", \"honour\", \"honour\", \"horses\", \"host\", \"host\", \"host\", \"hour\", \"hours\", \"house\", \"house\", \"house\", \"hubahib\", \"hud\", \"hud\", \"hud\", \"humankind\", \"humans\", \"husbands\", \"hymn\", \"hypocrites\", \"hypocrites\", \"hypocrites\", \"hypocrites\", \"ibn\", \"ibn\", \"ibn\", \"ibn\", \"ibn\", \"ibn\", \"idol\", \"idolaters\", \"idolaters\", \"idolaters\", \"idolaters\", \"idolaters\", \"idolaters\", \"idols\", \"idols\", \"idols\", \"idols\", \"il\", \"immortal\", \"independent\", \"inhabitants\", \"inhabitants\", \"inherit\", \"inheritance\", \"intense\", \"interpretation\", \"invented\", \"iron\", \"ishah\", \"jahl\", \"jesus\", \"jesus\", \"jews\", \"jews\", \"jinn\", \"jinn\", \"join\", \"joseph\", \"joseph\", \"judgement\", \"judgement\", \"judgement\", \"judgement\", \"judges\", \"keepers\", \"khalaf\", \"king\", \"king\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"labid\", \"land\", \"lands\", \"language\", \"latter\", \"latter\", \"latter\", \"latter\", \"leads\", \"leapeth\", \"led\", \"led\", \"led\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"life\", \"life\", \"life\", \"life\", \"lifted\", \"light\", \"light\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"listen\", \"live\", \"live\", \"living\", \"living\", \"living\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"loss\", \"love\", \"love\", \"love\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"magic\", \"magic\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"makhzumi\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"manaf\", \"mankind\", \"mankind\", \"mankind\", \"mankind\", \"mankind\", \"mankind\", \"mankind\", \"mary\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"mecca\", \"mecca\", \"mecca\", \"mecca\", \"mecca\", \"mecca\", \"mecca\", \"medina\", \"medina\", \"medina\", \"mercy\", \"mercy\", \"mercy\", \"mercy\", \"mercy\", \"mercy\", \"mercy\", \"merit\", \"merit\", \"merit\", \"messenger\", \"messenger\", \"messenger\", \"messenger\", \"messenger\", \"messengers\", \"messengers\", \"messengers\", \"military\", \"mim\", \"moon\", \"moses\", \"moses\", \"moses\", \"moses\", \"mother\", \"mother\", \"mount\", \"mount\", \"mountain\", \"mountain\", \"mountain\", \"moves\", \"mu\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muhammad\", \"muzdalifah\", \"nadr\", \"narration\", \"neck\", \"neck\", \"needy\", \"night\", \"night\", \"noah\", \"none\", \"none\", \"none\", \"none\", \"none\", \"obedience\", \"obedience\", \"obedience\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"olive\", \"olive\", \"olive\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"oneness\", \"oneness\", \"oppose\", \"opposition\", \"orphan\", \"orphan\", \"painful\", \"painful\", \"palm\", \"paradise\", \"paradise\", \"path\", \"path\", \"pbuh\", \"pbuh\", \"pbuh\", \"pbuh\", \"pbuh\", \"pbuh\", \"pbuh\", \"pbuh\", \"peace\", \"peace\", \"peace\", \"peace\", \"peace\", \"peace\", \"pen\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perfectly\", \"performing\", \"pharaoh\", \"pity\", \"please\", \"pleasure\", \"poor\", \"portent\", \"portent\", \"portents\", \"portents\", \"portion\", \"portion\", \"posterity\", \"poured\", \"power\", \"power\", \"power\", \"power\", \"praise\", \"praise\", \"praise\", \"praise\", \"praise\", \"praises\", \"pray\", \"pray\", \"pray\", \"pray\", \"prayer\", \"prayer\", \"prayer\", \"prayer\", \"prayers\", \"prayers\", \"prayers\", \"previous\", \"private\", \"privately\", \"profess\", \"profession\", \"prohibitions\", \"prohibitions\", \"prohibitions\", \"prohibitions\", \"prophet\", \"prophet\", \"prophet\", \"prophet\", \"prophet\", \"prophet\", \"prophet\", \"prophethood\", \"prophethood\", \"prophethood\", \"prostrate\", \"prostrate\", \"prostration\", \"prostration\", \"prostration\", \"protection\", \"proud\", \"proud\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"punishment\", \"purified\", \"puss\", \"quite\", \"qur\", \"qur\", \"qur\", \"qur\", \"qur\", \"qur\", \"qur\", \"qur\", \"quraysh\", \"quraysh\", \"quraysh\", \"quraysh\", \"quraysh\", \"qurt\", \"raging\", \"rain\", \"rank\", \"rebelled\", \"rebellious\", \"rebellious\", \"recited\", \"recorded\", \"records\", \"refers\", \"refers\", \"refers\", \"refers\", \"reflect\", \"refuge\", \"refuse\", \"regarding\", \"regarding\", \"regarding\", \"regarding\", \"regarding\", \"register\", \"registers\", \"relate\", \"relatives\", \"religion\", \"religion\", \"religion\", \"religion\", \"religion\", \"remembrance\", \"remembrance\", \"removed\", \"reply\", \"resemble\", \"revealed\", \"revealed\", \"revealed\", \"revealed\", \"revealed\", \"revealed\", \"revelations\", \"revelations\", \"revelations\", \"revelations\", \"revelations\", \"revelations\", \"reward\", \"reward\", \"reward\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"rivalry\", \"rooms\", \"safe\", \"safe\", \"safe\", \"safwan\", \"sahm\", \"sahmi\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"salih\", \"sam\", \"samad\", \"satan\", \"satan\", \"satan\", \"satan\", \"satan\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"saw\", \"saw\", \"saw\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"saying\", \"saying\", \"saying\", \"saying\", \"saying\", \"saying\", \"saying\", \"says\", \"says\", \"says\", \"says\", \"says\", \"says\", \"says\", \"says\", \"says\", \"scale\", \"scales\", \"scouring\", \"scripture\", \"scripture\", \"scripture\", \"scripture\", \"sea\", \"sea\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seek\", \"seek\", \"seek\", \"sees\", \"seeth\", \"seize\", \"sent\", \"sent\", \"sent\", \"sent\", \"sent\", \"sent\", \"serve\", \"severe\", \"severe\", \"severe\", \"shaken\", \"shakes\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"ships\", \"shu\", \"siddiq\", \"sign\", \"sign\", \"signs\", \"signs\", \"signs\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"sky\", \"slave\", \"slave\", \"slave\", \"small\", \"small\", \"snorting\", \"son\", \"son\", \"son\", \"son\", \"son\", \"soon\", \"sovereignty\", \"sovereignty\", \"sovereignty\", \"sparks\", \"speaks\", \"spend\", \"spending\", \"spent\", \"sperm\", \"staff\", \"stands\", \"step\", \"stones\", \"stopped\", \"stopping\", \"store\", \"stores\", \"straight\", \"succour\", \"summer\", \"sun\", \"supplications\", \"surface\", \"surrendered\", \"swears\", \"swears\", \"swears\", \"swore\", \"swore\", \"syria\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"taste\", \"taste\", \"taste\", \"teacheth\", \"term\", \"term\", \"thamud\", \"thee\", \"thee\", \"thee\", \"thee\", \"thee\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therein\", \"therein\", \"therein\", \"therein\", \"therein\", \"therein\", \"therein\", \"thereof\", \"thereof\", \"thereof\", \"thereof\", \"thereof\", \"thereof\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"thinketh\", \"thou\", \"thou\", \"thou\", \"thou\", \"thou\", \"thou\", \"threw\", \"throw\", \"thrown\", \"thy\", \"thy\", \"thy\", \"thy\", \"thy\", \"tightening\", \"toil\", \"torah\", \"torment\", \"torment\", \"torment\", \"torment\", \"torment\", \"torment\", \"tortured\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"travel\", \"treasures\", \"tree\", \"tree\", \"trees\", \"trees\", \"trees\", \"trees\", \"tremendousness\", \"tribe\", \"tribe\", \"troops\", \"truly\", \"truly\", \"trumpet\", \"truth\", \"truth\", \"truth\", \"truth\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"ubayy\", \"ubayy\", \"umayyah\", \"umayyah\", \"umm\", \"uncle\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"untied\", \"unto\", \"unto\", \"unto\", \"unto\", \"unto\", \"unto\", \"unto\", \"unto\", \"unto\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upper\", \"upright\", \"us\", \"us\", \"us\", \"us\", \"us\", \"usayd\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"utter\", \"vegetation\", \"verses\", \"verses\", \"voice\", \"wa\", \"wa\", \"waiting\", \"waiting\", \"walid\", \"walid\", \"walid\", \"ward\", \"ward\", \"ward\", \"warner\", \"warner\", \"warning\", \"warning\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"ways\", \"wealth\", \"wealth\", \"weighed\", \"weight\", \"whispers\", \"whoso\", \"whoso\", \"whoso\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wilt\", \"wine\", \"winter\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"witness\", \"witness\", \"witness\", \"wives\", \"wives\", \"wives\", \"woe\", \"wool\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"world\", \"world\", \"world\", \"worship\", \"worship\", \"worship\", \"worship\", \"worship\", \"worship\", \"worship\", \"worshippers\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wretched\", \"writing\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"years\", \"years\", \"years\", \"years\", \"yemen\", \"yes\", \"yes\", \"yieldeth\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 5, 6, 2, 9, 3, 4, 8, 1, 7]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el3951351703448004801806820287\", ldavis_el3951351703448004801806820287_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el3951351703448004801806820287\", ldavis_el3951351703448004801806820287_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el3951351703448004801806820287\", ldavis_el3951351703448004801806820287_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.save(\"quran_lda_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPoBN8Ka_rCg",
        "outputId": "d8bfb4f2-36ce-4d36-805f-0d715453ae8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_lda_model = LdaModel.load(\"quran_lda_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-CPG8Jc_tYh",
        "outputId": "31ef476a-8966-4a17-8103-f8166b4a8812"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_representative_docs(lda_model, corpus, num_docs=5):\n",
        "    for i in range(lda_model.num_topics):\n",
        "        print(f\"\\nTopic {i}:\")\n",
        "        for j, doc in enumerate(corpus):\n",
        "            if i in [topic_num for topic_num, prob in lda_model.get_document_topics(doc)]:\n",
        "                if j < num_docs:\n",
        "                    print(f\"Document {j}:\")\n",
        "                    print(column_data[j])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgTVU6N2_yN4",
        "outputId": "a3911440-3a92-44fb-d4c7-caa316acedd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn gensim pyLDAvis xlsxwriter nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOkOYBXLBfZ4",
        "outputId": "b7514f7f-476c-4f03-c82c-d50c6dd41196"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (71.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "\n",
        "def get_representative_docs(lda_model, corpus, num_docs_per_topic, excel_file_name):\n",
        "    workbook = xlsxwriter.Workbook(excel_file_name)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # Define column names\n",
        "    column_names = [\"Topic\", \"Document\", \"Original Text\"]\n",
        "    worksheet.write_row(0, 0, column_names)\n",
        "\n",
        "    row = 1  # Start writing data from the second row\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        # Get the top documents for each topic\n",
        "        topic_docs = []\n",
        "        for doc_idx, doc in enumerate(corpus):\n",
        "            topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
        "            topic_probability = dict(topic_distribution).get(i, 0)\n",
        "            topic_docs.append((topic_probability, doc_idx))\n",
        "\n",
        "        # Sort documents by probability and select the top ones\n",
        "        topic_docs = sorted(topic_docs, reverse=True)[:num_docs_per_topic]\n",
        "\n",
        "        for topic_prob, doc_idx in topic_docs:\n",
        "            # Write topic number, document index, and the original text to the Excel file\n",
        "            worksheet.write(row, 0, f\"Topic {i}\")\n",
        "            worksheet.write(row, 1, doc_idx)\n",
        "            worksheet.write(row, 2, column_data[doc_idx])\n",
        "            row += 1\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "# Call the function to save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, num_docs_per_topic=5, excel_file_name=\"representative_docs.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vUjmOdHBjil",
        "outputId": "cd18a7fb-c3cd-4e5f-deae-8b2d21e68bbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Reading the CSV file and defining the dictionaries\n",
        "tafseer_ayah = {}\n",
        "ayah_translation = {}\n",
        "ayah_verseNumber = {}\n",
        "\n",
        "with open(\"/content/main_df.csv\", \"r\") as f:  # Change this path to your CSV file location\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip the header row\n",
        "    for row in reader:\n",
        "        ayah_key = row[8]  # Adjust the index based on your CSV structure\n",
        "        tafseer_ayah[ayah_key] = row[3]  # Mapping ayah to tafseer\n",
        "        ayah_translation[ayah_key] = row[4]  # Mapping ayah to translation\n",
        "        ayah_verseNumber[ayah_key] = (row[1], row[2])  # Mapping ayah to verse number\n",
        "\n",
        "\n",
        "import xlsxwriter\n",
        "import pandas as pd\n",
        "\n",
        "def get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic, excel_file_name):\n",
        "    workbook = xlsxwriter.Workbook(excel_file_name)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # Define column names\n",
        "    column_names = [\"Topic\", \"Ayah\", \"Translation\", \"Verse Number\", \"Score\"]\n",
        "    worksheet.write_row(0, 0, column_names)\n",
        "\n",
        "    row = 1  # Start writing data from the second row\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        # Get the top documents for each topic\n",
        "        topic_docs = []\n",
        "        for doc_idx, doc in enumerate(corpus):\n",
        "            topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
        "            topic_probability = dict(topic_distribution).get(i, 0)\n",
        "            topic_docs.append((topic_probability, doc_idx))\n",
        "\n",
        "        # Sort documents by probability and select the top ones\n",
        "        topic_docs = sorted(topic_docs, reverse=True)[:num_docs_per_topic]\n",
        "\n",
        "        for topic_prob, doc_idx in topic_docs:\n",
        "            # Convert the bag of words (BoW) to the original text\n",
        "            bow = corpus[doc_idx]\n",
        "            original_text = \" \".join([dictionary[word_id] for word_id, freq in bow])\n",
        "\n",
        "            # Write topic keywords, original text, and other details to the Excel file\n",
        "            topic_keywords = [word for word, _ in lda_model.show_topic(i)]\n",
        "            ayah = tafseer_ayah.get(doc_idx, \"N/A\")  # Fetch ayah\n",
        "            translation = ayah_translation.get(ayah, \"N/A\")  # Fetch translation\n",
        "            verse_number = ayah_verseNumber.get(ayah, (\"N/A\", \"N/A\"))  # Fetch verse number\n",
        "\n",
        "            worksheet.write(row, 0, \", \".join(topic_keywords))  # Topic Keywords\n",
        "            worksheet.write(row, 1, ayah)  # Ayah\n",
        "            worksheet.write(row, 2, translation)  # Translation\n",
        "            worksheet.write(row, 3, str(verse_number))  # Verse Number\n",
        "            worksheet.write(row, 4, topic_prob)  # Score\n",
        "            row += 1\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "# Call the function to save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=5, excel_file_name=\"representative_docs.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cweJSrQRC1CX",
        "outputId": "4ff6cef2-11a5-4d0e-fc73-181765dc6d67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import xlsxwriter\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Step 1: Read the specific column from the CSV file\n",
        "def read_specific_column(csv_file, column_name):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if column_name in df.columns:\n",
        "        column_data = df[column_name].astype(str).tolist()\n",
        "        return column_data\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' not found in the CSV file.\")\n",
        "        return []\n",
        "\n",
        "# Load the Tafseer data\n",
        "csv_file_path = 'main_df.csv'  # Adjust this path to your CSV file location\n",
        "column_name_to_read = 'Tafaseer2'\n",
        "column_data = read_specific_column(csv_file_path, column_name_to_read)\n",
        "\n",
        "# Step 2: Prepare the dictionaries (tafseer_ayah, ayah_translation, ayah_verseNumber)\n",
        "tafseer_ayah = {}\n",
        "ayah_translation = {}\n",
        "ayah_verseNumber = {}\n",
        "\n",
        "with open(csv_file_path, \"r\") as f:  # Adjust this path to your CSV file location\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip the header row\n",
        "    for row in reader:\n",
        "        ayah_key = row[8]  # Adjust the index based on your CSV structure\n",
        "        tafseer_ayah[ayah_key] = row[3]  # Mapping ayah to tafseer\n",
        "        ayah_translation[ayah_key] = row[4]  # Mapping ayah to translation\n",
        "        ayah_verseNumber[ayah_key] = (row[1], row[2])  # Mapping ayah to verse number\n",
        "\n",
        "# Step 3: Tokenize the text data for LDA\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(column_data)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Step 4: Create the dictionary and corpus needed for LDA\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in column_data])\n",
        "corpus = [dictionary.doc2bow(doc.split()) for doc in column_data]\n",
        "\n",
        "# Step 5: Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=15)\n",
        "\n",
        "# Step 6: Extract representative documents and save to Excel\n",
        "def get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic, excel_file_name):\n",
        "    workbook = xlsxwriter.Workbook(excel_file_name)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # Define column names\n",
        "    column_names = [\"Topic\", \"Ayah\", \"Translation\", \"Verse Number\", \"Score\"]\n",
        "    worksheet.write_row(0, 0, column_names)\n",
        "\n",
        "    row = 1  # Start writing data from the second row\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        # Get the top documents for each topic\n",
        "        topic_docs = []\n",
        "        for doc_idx, doc in enumerate(corpus):\n",
        "            topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
        "            topic_probability = dict(topic_distribution).get(i, 0)\n",
        "            topic_docs.append((topic_probability, doc_idx))\n",
        "\n",
        "        # Sort documents by probability and select the top ones\n",
        "        topic_docs = sorted(topic_docs, reverse=True)[:num_docs_per_topic]\n",
        "\n",
        "        for topic_prob, doc_idx in topic_docs:\n",
        "            # Convert the bag of words (BoW) to the original text\n",
        "            bow = corpus[doc_idx]\n",
        "            original_text = \" \".join([dictionary[word_id] for word_id, freq in bow])\n",
        "\n",
        "            # Write topic keywords, original text, and other details to the Excel file\n",
        "            topic_keywords = [word for word, _ in lda_model.show_topic(i)]\n",
        "            ayah = tafseer_ayah.get(original_text, \"N/A\")  # Fetch ayah\n",
        "            translation = ayah_translation.get(ayah, \"N/A\")  # Fetch translation\n",
        "            verse_number = ayah_verseNumber.get(ayah, (\"N/A\", \"N/A\"))  # Fetch verse number\n",
        "\n",
        "            worksheet.write(row, 0, \", \".join(topic_keywords))  # Topic Keywords\n",
        "            worksheet.write(row, 1, ayah)  # Ayah\n",
        "            worksheet.write(row, 2, translation)  # Translation\n",
        "            worksheet.write(row, 3, str(verse_number))  # Verse Number\n",
        "            worksheet.write(row, 4, topic_prob)  # Score\n",
        "            row += 1\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "# Step 7: Save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=5, excel_file_name=\"representative_docs.xlsx\")\n",
        "\n",
        "print(\"Representative documents saved to Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v-SJxfFFpSK",
        "outputId": "64539799-b5de-4d17-e339-5bf60b8ecf57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative documents saved to Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import xlsxwriter\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Step 1: Read the specific column from the CSV file\n",
        "def read_specific_column(csv_file, column_name):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if column_name in df.columns:\n",
        "        column_data = df[column_name].astype(str).tolist()\n",
        "        return df, column_data\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' not found in the CSV file.\")\n",
        "        return None, []\n",
        "\n",
        "# Load the Tafseer data\n",
        "csv_file_path = 'main_df.csv'  # Adjust this path to your CSV file location\n",
        "column_name_to_read = 'Tafaseer2'\n",
        "df, column_data = read_specific_column(csv_file_path, column_name_to_read)\n",
        "\n",
        "# Step 2: Prepare the dictionaries (tafseer_ayah, ayah_translation, ayah_verseNumber)\n",
        "tafseer_ayah = {}\n",
        "ayah_translation = {}\n",
        "ayah_verseNumber = {}\n",
        "\n",
        "with open(csv_file_path, \"r\") as f:  # Adjust this path to your CSV file location\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # Capture the header\n",
        "    for row in reader:\n",
        "        ayah_key = row[8]  # Adjust the index based on your CSV structure\n",
        "        tafseer_ayah[ayah_key] = row[3]  # Mapping ayah to tafseer\n",
        "        ayah_translation[ayah_key] = row[4]  # Mapping ayah to translation\n",
        "        ayah_verseNumber[ayah_key] = (row[1], row[2])  # Mapping ayah to verse number\n",
        "\n",
        "# Step 3: Tokenize the text data for LDA\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(column_data)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Step 4: Create the dictionary and corpus needed for LDA\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in column_data])\n",
        "corpus = [dictionary.doc2bow(doc.split()) for doc in column_data]\n",
        "\n",
        "# Step 5: Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=15)\n",
        "\n",
        "# Step 6: Extract representative documents and save to Excel\n",
        "def get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic, excel_file_name, df):\n",
        "    workbook = xlsxwriter.Workbook(excel_file_name)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # Define column names\n",
        "    column_names = [\"Topic\", \"Ayah\", \"Translation\", \"Verse Number\", \"Score\"]\n",
        "    worksheet.write_row(0, 0, column_names)\n",
        "\n",
        "    row = 1  # Start writing data from the second row\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        # Get the top documents for each topic\n",
        "        topic_docs = []\n",
        "        for doc_idx, doc in enumerate(corpus):\n",
        "            topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
        "            topic_probability = dict(topic_distribution).get(i, 0)\n",
        "            topic_docs.append((topic_probability, doc_idx))\n",
        "\n",
        "        # Sort documents by probability and select the top ones\n",
        "        topic_docs = sorted(topic_docs, reverse=True)[:num_docs_per_topic]\n",
        "\n",
        "        for topic_prob, doc_idx in topic_docs:\n",
        "            # Convert the bag of words (BoW) to the original text\n",
        "            bow = corpus[doc_idx]\n",
        "            original_text = \" \".join([dictionary[word_id] for word_id, freq in bow])\n",
        "\n",
        "            # Write topic keywords, original text, and other details to the Excel file\n",
        "            topic_keywords = [word for word, _ in lda_model.show_topic(i)]\n",
        "            ayah_row = df.iloc[doc_idx]  # Use the row directly from the DataFrame\n",
        "\n",
        "            ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
        "            translation = ayah_translation.get(ayah, \"N/A\")  # Fetch translation\n",
        "            verse_number = ayah_verseNumber.get(ayah, (\"N/A\", \"N/A\"))  # Fetch verse number\n",
        "\n",
        "            worksheet.write(row, 0, \", \".join(topic_keywords))  # Topic Keywords\n",
        "            worksheet.write(row, 1, ayah_row[3])  # Ayah\n",
        "            worksheet.write(row, 2, translation)  # Translation\n",
        "            worksheet.write(row, 3, str(verse_number))  # Verse Number\n",
        "            worksheet.write(row, 4, topic_prob)  # Score\n",
        "            row += 1\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "# Step 7: Save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=7, excel_file_name=\"representative_docs.xlsx\", df=df)\n",
        "\n",
        "print(\"Representative documents saved to Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDxtXWHAGlv-",
        "outputId": "5947ff39-8148-4de7-c7d2-3098ff9a8277"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-15-3b3675ce8e82>:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
            "<ipython-input-15-3b3675ce8e82>:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  worksheet.write(row, 1, ayah_row[3])  # Ayah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative documents saved to Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=7, excel_file_name=\"representative_docs.xlsx\", df=df)\n",
        "\n",
        "print(\"Representative documents saved to Excel file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUI0VkZhHheU",
        "outputId": "10e2d001-0f44-48e6-e716-00924d021748"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-15-3b3675ce8e82>:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
            "<ipython-input-15-3b3675ce8e82>:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  worksheet.write(row, 1, ayah_row[3])  # Ayah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative documents saved to Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import xlsxwriter\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS, preprocess_string, strip_punctuation, strip_numeric, strip_short\n",
        "\n",
        "# Step 1: Read the specific column from the CSV file\n",
        "def read_specific_column(csv_file, column_name):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if column_name in df.columns:\n",
        "        column_data = df[column_name].astype(str).tolist()\n",
        "        return df, column_data\n",
        "    else:\n",
        "        print(f\"Column '{column_name}' not found in the CSV file.\")\n",
        "        return None, []\n",
        "\n",
        "# Load the Tafseer data\n",
        "csv_file_path = 'main_df.csv'  # Adjust this path to your CSV file location\n",
        "column_name_to_read = 'Tafaseer2'\n",
        "df, column_data = read_specific_column(csv_file_path, column_name_to_read)\n",
        "\n",
        "# Step 2: Prepare the dictionaries (tafseer_ayah, ayah_translation, ayah_verseNumber)\n",
        "tafseer_ayah = {}\n",
        "ayah_translation = {}\n",
        "ayah_verseNumber = {}\n",
        "\n",
        "with open(csv_file_path, \"r\") as f:  # Adjust this path to your CSV file location\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # Capture the header\n",
        "    for row in reader:\n",
        "        ayah_key = row[8]  # Adjust the index based on your CSV structure\n",
        "        tafseer_ayah[ayah_key] = row[3]  # Mapping ayah to tafseer\n",
        "        ayah_translation[ayah_key] = row[4]  # Mapping ayah to translation\n",
        "        ayah_verseNumber[ayah_key] = (row[1], row[2])  # Mapping ayah to verse number\n",
        "\n",
        "# Step 3: Advanced Preprocessing\n",
        "def preprocess_text(text):\n",
        "    custom_filters = [strip_punctuation, strip_numeric, strip_short, lambda x: x.lower()]\n",
        "    processed_text = preprocess_string(text, custom_filters)\n",
        "    processed_text = [word for word in processed_text if word not in STOPWORDS]\n",
        "    return \" \".join(processed_text)\n",
        "\n",
        "# Apply preprocessing to all documents\n",
        "processed_column_data = [preprocess_text(doc) for doc in column_data]\n",
        "\n",
        "# Step 4: Create the dictionary and corpus needed for LDA\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in processed_column_data])\n",
        "corpus = [dictionary.doc2bow(doc.split()) for doc in processed_column_data]\n",
        "\n",
        "# Step 5: Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=15)\n",
        "\n",
        "# Step 6: Extract representative documents and save to Excel\n",
        "def get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic, excel_file_name, df):\n",
        "    workbook = xlsxwriter.Workbook(excel_file_name)\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # Define column names\n",
        "    column_names = [\"Topic\", \"Ayah\", \"Translation\", \"Verse Number\", \"Score\"]\n",
        "    worksheet.write_row(0, 0, column_names)\n",
        "\n",
        "    row = 1  # Start writing data from the second row\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        # Get the top documents for each topic\n",
        "        topic_docs = []\n",
        "        for doc_idx, doc in enumerate(corpus):\n",
        "            topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
        "            topic_probability = dict(topic_distribution).get(i, 0)\n",
        "            topic_docs.append((topic_probability, doc_idx))\n",
        "\n",
        "        # Sort documents by probability and select the top ones\n",
        "        topic_docs = sorted(topic_docs, reverse=True)[:num_docs_per_topic]\n",
        "\n",
        "        for topic_prob, doc_idx in topic_docs:\n",
        "            # Convert the bag of words (BoW) to the original text\n",
        "            bow = corpus[doc_idx]\n",
        "            original_text = \" \".join([dictionary[word_id] for word_id, freq in bow])\n",
        "\n",
        "            # Write topic keywords, original text, and other details to the Excel file\n",
        "            topic_keywords = [word for word, _ in lda_model.show_topic(i)]\n",
        "            ayah_row = df.iloc[doc_idx]  # Use the row directly from the DataFrame\n",
        "\n",
        "            ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
        "            translation = ayah_translation.get(ayah, \"N/A\")  # Fetch translation\n",
        "            verse_number = ayah_verseNumber.get(ayah, (\"N/A\", \"N/A\"))  # Fetch verse number\n",
        "\n",
        "            worksheet.write(row, 0, \", \".join(topic_keywords))  # Topic Keywords\n",
        "            worksheet.write(row, 1, ayah_row[3])  # Ayah\n",
        "            worksheet.write(row, 2, translation)  # Translation\n",
        "            worksheet.write(row, 3, str(verse_number))  # Verse Number\n",
        "            worksheet.write(row, 4, topic_prob)  # Score\n",
        "            row += 1\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "# Step 7: Save 5 representative documents per topic to an Excel file\n",
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=5, excel_file_name=\"representative_docs.xlsx\", df=df)\n",
        "\n",
        "print(\"Representative documents saved to Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqiSZc-QItcB",
        "outputId": "c3d62702-6d59-4bdd-e362-abcb4caa41ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-17-64570de2f1fa>:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
            "<ipython-input-17-64570de2f1fa>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  worksheet.write(row, 1, ayah_row[3])  # Ayah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative documents saved to Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_representative_docs(lda_model, corpus, dictionary, num_docs_per_topic=7, excel_file_name=\"representative_docs.xlsx\", df=df)\n",
        "\n",
        "print(\"Representative documents saved to Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8rm6fieJMBp",
        "outputId": "a167dbcb-19cd-43c6-918b-131e98d50759"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-17-64570de2f1fa>:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  ayah = ayah_row[8]  # Use the column that maps to the Ayah\n",
            "<ipython-input-17-64570de2f1fa>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  worksheet.write(row, 1, ayah_row[3])  # Ayah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative documents saved to Excel file.\n"
          ]
        }
      ]
    }
  ]
}